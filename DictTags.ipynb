{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import sys\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Tagging of Maintenance Issues:\n",
    "## A Keyword Detection and Ranking Approach\n",
    "### Thurston Sexton + Mike Brundage\n",
    "Also: compared to a Machine Learning one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_directory = os.path.join('.', 'data')\n",
    "raw_excel_filepath = os.path.join(data_directory, 'tag_data.xlsx')\n",
    "raw_csv_filepath = os.path.join(data_directory, 'raw_csv_tagged.csv')\n",
    "vocab_filepath = os.path.join(data_directory, 'tag_vocab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Monitoring Spindle Drive alarm wonaTMt clear. Replaced performance module'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the tagged issues, which have been cleaned of nasty unicode and combined Description+Resolution\n",
    "df_raw = pd.read_csv(raw_csv_filepath, encoding=sys.getfilesystemencoding(), \n",
    "                     names = ['RawText','p_Item','p_Action','s_Action','s_Item'])\n",
    "df_raw.head()\n",
    "df_raw.iloc[142].RawText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NE</th>\n",
       "      <th>alias</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>replace</th>\n",
       "      <td>S</td>\n",
       "      <td>replace</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit</th>\n",
       "      <td>I</td>\n",
       "      <td>unit</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motor</th>\n",
       "      <td>I</td>\n",
       "      <td>motor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spindle</th>\n",
       "      <td>I</td>\n",
       "      <td>spindle</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leak</th>\n",
       "      <td>P</td>\n",
       "      <td>leak</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        NE    alias note\n",
       "token                   \n",
       "replace  S  replace  NaN\n",
       "unit     I     unit  NaN\n",
       "motor    I    motor  NaN\n",
       "spindle  I  spindle  NaN\n",
       "leak     P     leak  NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the keyword dictionary and to some data-munging. \n",
    "df_vocab = pd.read_csv(vocab_filepath, header=0, encoding=sys.getfilesystemencoding(),\n",
    "                       names = ['token', 'NE','alias','note'], index_col=0)\n",
    "df_vocab = df_vocab.dropna(subset=['NE'])  # remove named entities that are NaN\n",
    "df_vocab.alias = df_vocab.apply(lambda x: np.where(pd.isnull(x.alias), x.name, x.alias), axis=1) # alias to original if blank\n",
    "df_vocab = df_vocab[~df_vocab.index.duplicated(keep='first')]\n",
    "df_vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NE</th>\n",
       "      <th>alias</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>1</td>\n",
       "      <td>366</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    NE  alias  note\n",
       "NE                 \n",
       "I    1    366    25\n",
       "P    1     95    14\n",
       "R    1    142     0\n",
       "S    1     70    11\n",
       "X    1     54     3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many instances of each keyword class are there?\n",
    "df_vocab.groupby(\"NE\").nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start up our NLP engine, Spacy wrapped in Textacy\n",
    "docs = textacy.fileio.read.read_csv(raw_csv_filepath, encoding='utf-8')\n",
    "\n",
    "content_stream, metadata_stream = textacy.fileio.split_record_fields(docs, 1)  # Descriptions in Col 6\n",
    "corpus = textacy.Corpus(u'en', texts=content_stream, metadatas=metadata_stream)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## THIS GENERATED THE TOP N MOST IMPORTANT TOKENS VIA A DOC_TERM_MATRIX\n",
    "## the engineers used this code-snippet to make tag_vocab.csv\n",
    "\n",
    "# from unicodedata import normalize\n",
    "\n",
    "# topn = 3000\n",
    "# topn_tok = [id2term[i] for i in doc_term_matrix.sum(axis=0).argsort()[0,-topn:].tolist()[0][::-1]]\n",
    "# with open('new_top{}.txt'.format(topn), 'wb') as f:\n",
    "#     for i in topn_tok:\n",
    "#         try:\n",
    "#             f.write(i+'\\n')\n",
    "#         except UnicodeEncodeError:\n",
    "#             print i, '-->', normalize('NFKD', i).encode('ascii','ignore')\n",
    "#             f.write(normalize('NFKD', i).encode('ascii','ignore') +'\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bd4dc1f45447d7b9696102aaec4752"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RawText</th>\n",
       "      <th>Items</th>\n",
       "      <th>Problem</th>\n",
       "      <th>Solution</th>\n",
       "      <th>UK_tok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No power. Replaced pin in pendant and powered ...</td>\n",
       "      <td>machine, cable, pendant, pin</td>\n",
       "      <td>short, power</td>\n",
       "      <td>replace</td>\n",
       "      <td>possible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Smartscope harness broken. Parts ordered / Tec...</td>\n",
       "      <td>part</td>\n",
       "      <td>broken</td>\n",
       "      <td>repair, order</td>\n",
       "      <td>harness, tech, smartscope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Check / Charge Accumulators. Where OK</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>charge, check</td>\n",
       "      <td>accumulators</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hyd leak at saw atachment. Replaced seal in sa...</td>\n",
       "      <td>seal, saw, attachment, hydraulic, saw_attachment</td>\n",
       "      <td>leak</td>\n",
       "      <td>replace</td>\n",
       "      <td>ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CS1008 setup change over / from ARC1004. Compl...</td>\n",
       "      <td>unit, thread, thread_unit</td>\n",
       "      <td></td>\n",
       "      <td>setup, complete, change</td>\n",
       "      <td>rewire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             RawText  \\\n",
       "0  No power. Replaced pin in pendant and powered ...   \n",
       "1  Smartscope harness broken. Parts ordered / Tec...   \n",
       "2              Check / Charge Accumulators. Where OK   \n",
       "3  Hyd leak at saw atachment. Replaced seal in sa...   \n",
       "4  CS1008 setup change over / from ARC1004. Compl...   \n",
       "\n",
       "                                              Items       Problem  \\\n",
       "0                      machine, cable, pendant, pin  short, power   \n",
       "1                                              part        broken   \n",
       "2                                                                   \n",
       "3  seal, saw, attachment, hydraulic, saw_attachment          leak   \n",
       "4                         unit, thread, thread_unit                 \n",
       "\n",
       "                  Solution                     UK_tok  \n",
       "0                  replace                   possible  \n",
       "1            repair, order  harness, tech, smartscope  \n",
       "2            charge, check               accumulators  \n",
       "3                  replace                         ml  \n",
       "4  setup, complete, change                     rewire  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_norm_tokens(doc_n, doc_term_mat, id2term):\n",
    "    doc = doc_term_mat[doc_n].toarray()\n",
    "    return [id2term[i] for i in doc.nonzero()[1]]\n",
    "\n",
    "def doc_to_tags(tokens, thes):\n",
    "#     tokens = get_norm_terms(doc)\n",
    "    tags = {'I':[], 'P':[], 'S':[]}\n",
    "    untagged = []\n",
    "    vocab_list = thes.index.tolist()\n",
    "    for tok in tokens:\n",
    "        if tok in vocab_list:  # recognized token?\n",
    "            typ = thes.loc[tok]['NE']\n",
    "            \n",
    "            if typ in tags.keys():  #  I, P, or S?\n",
    "                tags[typ] = list(set(tags[typ] + [thes.loc[tok]['alias'].tolist()]))\n",
    "            else:  # R or X?\n",
    "                pass # skip'em\n",
    "        elif np.any([i in vocab_list for i in tok.split(' ')]):\n",
    "            # If any subset of `tok` is itself a recognized token, skip'em\n",
    "            pass\n",
    "        else: # not recognized :(\n",
    "            untagged += [tok]\n",
    "    return tags, list(set(untagged))\n",
    "        \n",
    "            \n",
    "def tag_corpus(corpus, thes):\n",
    "    RT, I, S, P, UK = ([], [], [], [], [])\n",
    "    \n",
    "    # make the tf-idf embedding to tokenize with lemma/ngrams\n",
    "    doc_term_matrix, id2term = textacy.vsm.doc_term_matrix(\n",
    "            (doc.to_terms_list(ngrams=(1,2,3), \n",
    "                               normalize=u'lemma',\n",
    "                               named_entities=False, \n",
    "#                                filter_stops=True,  # Nope! Not needed :)\n",
    "                               filter_punct=True,\n",
    "                               as_strings=True)\n",
    "                for doc in corpus),\n",
    "            weighting='tfidf', \n",
    "            normalize=False, \n",
    "            smooth_idf=False, \n",
    "            min_df=2, max_df=0.95)  # each token in >2 docs, <95% of docs\n",
    "    # iterate over all issues\n",
    "    for  doc_n, doc in tqdm_notebook(enumerate(corpus)):\n",
    "        tokens = get_norm_tokens(doc_n, doc_term_matrix, id2term)\n",
    "        tags, unknown = doc_to_tags(tokens, thes)\n",
    "        UK += [', '.join(unknown)]\n",
    "        RT += [doc.text]\n",
    "        I += [', '.join(tags['I'])]\n",
    "        S += [', '.join(tags['S'])]\n",
    "        P += [', '.join(tags['P'])]\n",
    "    # get back a tagged DF\n",
    "    return pd.DataFrame(data={\n",
    "        'RawText': RT,\n",
    "        'Items': I,\n",
    "        'Problem': P,\n",
    "        'Solution': S,\n",
    "        'UK_tok': UK  # unknown\n",
    "    }, columns = ['RawText','Items','Problem','Solution','UK_tok'])\n",
    "\n",
    "df_pred = tag_corpus(corpus, df_vocab)\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save everything to disk\n",
    "df_pred.to_excel('keyword_tagged.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many have no remaining Unknown Tokens?\n",
    "i.e. the mapping from token-space (domain) --> tag-space (codomain) is a surjection in the space of this issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred[df_pred.UK_tok ==''].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the 3438-773 = 2665 others *apparently* have extra information to be extracted, or at least, *we cannot be certain that there isn't*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many got NO datafication?\n",
    "i.e. for how many issues was this process completely worthless? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RawText</th>\n",
       "      <th>Items</th>\n",
       "      <th>Problem</th>\n",
       "      <th>Solution</th>\n",
       "      <th>UK_tok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>Unload automation not returning.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>unload, return, automation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>Saftey paint required on platform.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>platform, require, paint require, paint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>Camshaft standstill. Gary!!</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>camshaft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>Dead spots on touch screen; Not always functio...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>screen, dead spot, spot, functional, dead, tou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3191</th>\n",
       "      <td>Disti water empty. Water filled;</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>water, disti, fill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405</th>\n",
       "      <td>??.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                RawText Items Problem  \\\n",
       "630                    Unload automation not returning.                 \n",
       "1816                                                  .                 \n",
       "2318                 Saftey paint required on platform.                 \n",
       "2467                                                  .                 \n",
       "2571                        Camshaft standstill. Gary!!                 \n",
       "2834  Dead spots on touch screen; Not always functio...                 \n",
       "3191                   Disti water empty. Water filled;                 \n",
       "3405                                                ??.                 \n",
       "\n",
       "     Solution                                             UK_tok  \n",
       "630                                   unload, return, automation  \n",
       "1816                                                              \n",
       "2318                     platform, require, paint require, paint  \n",
       "2467                                                              \n",
       "2571                                                    camshaft  \n",
       "2834           screen, dead spot, spot, functional, dead, tou...  \n",
       "3191                                          water, disti, fill  \n",
       "3405                                                              "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_pred[df_pred.UK_tok ==''].dropna().shape\n",
    "print df_pred[(df_pred[['Items','Problem','Solution']]=='').all(axis=1)].shape[0]\n",
    "df_pred[(df_pred[['Items','Problem','Solution']]=='').all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well did we do at automating the job of Tagging Issues?\n",
    "\n",
    "In order to somehow measure our success, we need something to compare with. Thankfully, some hard-working engineers have gone through and manually tagged over 1200 maintenance issues **by hand**. We can use these tags as the \"gold standard\" tags, with which to compare our automated keyword-->tag mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'pendant_cable', u'cable', u'short', u'no_power', u'replace', u'pin']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "df_tag = pd.read_excel(raw_excel_filepath, header=1, encoding=sys.getfilesystemencoding(), \n",
    "                   names=['Description','Resolution','p_Item','p_Action','s_Action','s_Item'])\n",
    "mask = df_tag[['p_Item','p_Action','s_Action','s_Item']].notnull().any(axis=1)\n",
    "\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "human_tags = [list(chain(*[tags.split(', ') for tags in issue[1].values if type(tags)==unicode])) \n",
    "              for issue in df_tag[['p_Item','p_Action','s_Action','s_Item']][mask].iterrows()]\n",
    "\n",
    "recog_tags =  [list(chain(*[tags.split(', ') for tags in issue[1].values if type(tags)==unicode])) \n",
    "              for issue in df_pred[['Items','Problem','Solution']][mask].iterrows()]\n",
    "\n",
    "print human_tags[0]\n",
    "\n",
    "\n",
    "multi_bin =  MultiLabelBinarizer().fit(human_tags + recog_tags)\n",
    "Y_true = multi_bin.transform(human_tags)\n",
    "print Y_true[0].sum()\n",
    "\n",
    "Y_train = multi_bin.transform(recog_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But how will we measure this? \n",
    "\n",
    "One way is straight-forward and easy to compute...the accuracy. The **accuracy score** measures, on average, how many predicted outputs match the true outputs perfectly:\n",
    "\n",
    "$$ S_A = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{1}(T_i = P_i)$$\n",
    "\n",
    "#### A better alternative :\n",
    "\n",
    "This is an *overly-harsh metric* for the performance of multilabel classification, since it ignores *how close* we got to the correct output in each case. The **hamming score** is a more forgiving and/or useful metric:\n",
    "\n",
    "$$ S_H = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{|T_i \\cap P_i|}{|T_i \\cup P_i|}$$\n",
    "\n",
    "Note that the closely related **hamming loss** is similar to a distance metric, which unlike the others here is *better when low*.\n",
    "\n",
    "#### Interpretability, please: \n",
    "Finally, we can use the slightly more intuitive **precision**, **recall**, and their harmonic mean **$F_1$-score**. As put by Scikit-Learn: \n",
    "> Intuitively, *precision* is the ability of the classifier *not to label as positive a sample that is negative*, and *recall* is the ability of the classifier *to find all the positive samples*.\n",
    "\n",
    "Then, we can get some sort of combination that balances the two, embodied by $F_1$. Or, put formally:\n",
    "\n",
    "$$ Pr = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{|T_i \\cap P_i|}{|P_i|} $$\n",
    "$$ Re = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{|T_i \\cap P_i|}{|T_i|} $$\n",
    "$$ F_1 = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{2Pr_i Re_i}{Pr_i+Re_i}$$\n",
    "\n",
    "If we want to model some difference between our importance of recall vs. precision, the generalized $F$-score is defined as:\n",
    "\n",
    "$$ F_{\\beta} = \\frac{1}{n}\\sum_{i=1}^{n} (1+\\beta^2)\\frac{Pr_i Re_i}{\\beta^2 Pr_i+Re_i} $$\n",
    "\n",
    "From  Van Rijsbergen, this is defined so that $F_{\\beta}$:\n",
    "> \"measures the effectiveness of retrieval with respect to a user who attaches $\\beta$ times as much importance to recall as precision\".\n",
    "\n",
    "In our case, Since we do not really trust that the original tags given by humans were all-inclusive (i.e. they might have left out tags below some un-known relevance threshold determined by their attention [read: boredom] level), we want to place more importance on recall in our $F$-measure. We'll use the commonly-applied $F_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Automatic Keyword Tagging (TF-IDF+human) ---\n",
      "Accuracy Score: \t 2.16%\n",
      "Hamming Score: \t 39.06%\n",
      "Hamming Loss: \t 3.82e-03\n",
      "\n",
      "Precision: \t 46.51%\n",
      "Recall: \t 68.23%\n",
      "F1 Score: \t 59.05%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss, accuracy_score, precision_recall_fscore_support\n",
    "from scipy.stats import hmean  # harmonic mean\n",
    "\n",
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    '''\n",
    "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
    "    http://stackoverflow.com/q/32239577/395857\n",
    "    '''\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        #print('\\nset_true: {0}'.format(set_true))\n",
    "        #print('set_pred: {0}'.format(set_pred))\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        #print('tmp_a: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)\n",
    "\n",
    "def f_score(y_true, y_pred, beta=1.):\n",
    "    '''\n",
    "    Compute the Precision, Recall, and F-beta Score for the multi-label case\n",
    "    http://stackoverflow.com/q/32239577/395857\n",
    "    '''\n",
    "    fsc_list = []\n",
    "    pre_list = []\n",
    "    rec_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        #print('\\nset_true: {0}'.format(set_true))\n",
    "        #print('set_pred: {0}'.format(set_pred))\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_p, tmp_r, tmp_f = 1, 1, 1\n",
    "        elif len(set_true.intersection(set_pred)) ==0:\n",
    "            tmp_p = 0\n",
    "            tmp_r = 0\n",
    "            tmp_f = 0\n",
    "        else:\n",
    "            tmp_p = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_pred) ) \n",
    "            tmp_r = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true) )\n",
    "            try:\n",
    "                tmp_f = ((1.+beta**2)*tmp_p*tmp_r)/((beta**2)*tmp_p + tmp_r)\n",
    "            except ValueError:\n",
    "                print tmp_p, tmp_r\n",
    "                raise\n",
    "        #print('tmp_a: {0}'.format(tmp_a))\n",
    "        pre_list.append(tmp_p)\n",
    "        rec_list.append(tmp_r)\n",
    "        fsc_list.append(tmp_f)\n",
    "    return np.array(pre_list), np.array(rec_list), np.array(fsc_list)\n",
    "\n",
    "print '---Automatic Keyword Tagging (TF-IDF+human) ---'\n",
    "print 'Accuracy Score: \\t {:.2%}\\nHamming Score: \\t {:.2%}\\nHamming Loss: \\t {:.2e}'.format(accuracy_score(Y_true, Y_train),\n",
    "                                                                          hamming_score(Y_true, Y_train),\n",
    "                                                                          hamming_loss(Y_true, Y_train))\n",
    "\n",
    "print '\\nPrecision: \\t {:.2%}\\nRecall: \\t {:.2%}\\nF1 Score: \\t {:.2%}'.format(\n",
    "    *[np.mean(i) for i in f_score(Y_true, Y_train, beta=2.)]\n",
    ")\n",
    "# print '\\nPrecision: \\t {:.2%}\\nRecall: \\t {:.2%}\\nF1 Score: \\t {:.2%}'.format(\n",
    "#     *[np.mean(i) for i in precision_recall_fscore_support(Y_true, Y_train)[:-1]]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there another way? \n",
    "\n",
    "Now, it's important to note that all of the above was done **only** with a list of categorized keywords (i.e. some mapping from token-space to tag-space), and creating that mapping did not at all depend upon some human having tagged *individual issues* already...we were just using those tagged issues as a scoring/benchmark tool. \n",
    "\n",
    "If we approach this as a classification problem, assuming these tagged issues *do exist*, we might attempt to train a classifier to predict the set of tags appropriate for given **raw-english** input.\n",
    "\n",
    "Let's try this: \n",
    "- first a mapping from token-space to some useful vector-space (could be tf-idf, maybe a topic model, but here we will use the shiny new **Word2Vec** semantic embedding vectors of our corpus, courtesy of Google+Textacy/Spacy). \n",
    "- Then we will train a **classifier** to exactly match the **Multilabel** output, represented by the individual human-tagged issues. \n",
    "\n",
    "Support-vector-machines work amazingly well on text embedding classification jobs, so let's use a linear SVC trained using stochasic gradient descent (SGD) via sklearn. We should also minimize overfitting with this hugely dimensional job, so we'll regularize with an elasticnet penalty (L1+L2).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "X_train = corpus.vectors[mask]\n",
    "clf = OneVsRestClassifier(SGDClassifier(#class_weight='balanced',  # compensate for class freqs\n",
    "                                        penalty='elasticnet'   # L1 + L2 regularized\n",
    "                                        ), n_jobs=3  # 3-cores for the one-vs-all \n",
    "                         )\n",
    "clf.fit(X_train, Y_true)\n",
    "Y_train_w2v = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---W2V Embeddings with SVC (OneVsRest, SGD)---\n",
      "Accuracy Score: \t 23.58%\n",
      "Hamming Score: \t 60.48%\n",
      "Hamming Loss: \t 1.91e-03\n",
      "\n",
      "Precision: \t 79.48%\n",
      "Recall: \t 71.30%\n",
      "F1 Score: \t 70.12%\n"
     ]
    }
   ],
   "source": [
    "print '---W2V Embeddings with SVC (OneVsRest, SGD)---'\n",
    "print 'Accuracy Score: \\t {:.2%}\\nHamming Score: \\t {:.2%}\\nHamming Loss: \\t {:.2e}'.format(accuracy_score(Y_true, Y_train_w2v),\n",
    "                                                                          hamming_score(Y_true, Y_train_w2v),\n",
    "                                                                          hamming_loss(Y_true, Y_train_w2v))\n",
    "print '\\nPrecision: \\t {:.2%}\\nRecall: \\t {:.2%}\\nF1 Score: \\t {:.2%}'.format(\n",
    "    *[np.mean(i) for i in f_score(Y_true, Y_train_w2v, beta=2.)]\n",
    ")\n",
    "# print '\\nPrecision: \\t {:.2%}\\nRecall: \\t {:.2%}\\nF1 Score: \\t {:.2%}'.format(\n",
    "#     *[np.mean(i) for i in precision_recall_fscore_support(Y_true, Y_train_w2v)[:-1]]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's pretty fantastic, considering the task we've set before it! Better performance, at least on a per-metric level, than our automated keyword-tagger in every way! \n",
    "\n",
    "Still, note that the precision is almost 80%...this may or may not be a model we want to actually use early on, given that we may or may not trust our engineers' tagging job. Assuming We 100% trust the keyword flagger when it recognizes a word, the precision of that one is actually 100%, and any discrepancy is on the part of *the original tags*. \n",
    "\n",
    "Another way to look at it is our keyword tagger is being really nit-picky and overly-accurate, which precision is punishing. Our classifier, on the other hand, is being trained *to tag like the humans*. \n",
    "\n",
    "Let's dig in a bit.. To get a more fine-grained idea of what's going on, we can also look at the Pr/Re/F scores on an individual-issue level to get a better idea of the performance of each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue No.  0\n",
      "No power. Replaced pin in pendant and powered machine; Possible short in pendant cable\n",
      "\n",
      "Human-tagged \"True\" Keyworks/Tags: \tcable, no_power, pendant_cable, pin, replace, short\n",
      "\n",
      "TF-IDF rank Human-classified keywords: \tcable, machine, pendant, pin, power, replace, short\n",
      "Precision: \t 57.14%\n",
      "Recall: \t 66.67%\n",
      "F2 Score: \t 64.52%\n",
      "\n",
      "Word2Vec + SVM Multilabel Classifier: \tcable, error, no_power, pin, replace\n",
      "Precision: \t 80.00%\n",
      "Recall: \t 66.67%\n",
      "F2 Score: \t 68.97%\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "from ipywidgets import interact, interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def compare_by_issue(iss):\n",
    "    print 'Issue No. ',iss\n",
    "    print df_pred[mask]['RawText'].iloc[iss]\n",
    "    print '\\nHuman-tagged \\\"True\\\" Keyworks/Tags: \\t{}'.format(', '.join(sorted(multi_bin.inverse_transform(Y_true)[iss])))\n",
    "\n",
    "    print '\\nTF-IDF rank Human-classified keywords: \\t{}'.format(', '.join(sorted(multi_bin.inverse_transform(Y_train)[iss])))\n",
    "    print 'Precision: \\t {:.2%}\\nRecall: \\t {:.2%}\\nF2 Score: \\t {:.2%}'.format(\n",
    "        *[i[iss] for i in f_score(Y_true, Y_train, beta=2.)]\n",
    "    )\n",
    "    print '\\nWord2Vec + SVM Multilabel Classifier: \\t{}'.format(', '.join(sorted(multi_bin.inverse_transform(Y_train_w2v)[iss])))\n",
    "    print 'Precision: \\t {:.2%}\\nRecall: \\t {:.2%}\\nF2 Score: \\t {:.2%}'.format(\n",
    "        *[i[iss] for i in f_score(Y_true, Y_train_w2v, beta=2.)]\n",
    "    )\n",
    "compare_by_issue(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5627749a4734067bd8753c6869bfff6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(compare_by_issue, iss=widgets.BoundedIntText(\n",
    "                    value=0,\n",
    "                    min=0,\n",
    "                    max=3437,\n",
    "                    description='Pick an Issue No.:',\n",
    "                    step=1\n",
    "                    )\n",
    "               );\n",
    "# display(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0657313987a047cf9a4b06a7f7562c37": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "HTMLModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "layout": "IPY_MODEL_c5428b7259c44b02853aa783c60a56ef",
       "value": "3438it [00:08, 404.33it/s]"
      }
     },
     "3658ab89cef348159fc37268c83286c7": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "OutputModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "jupyter-js-widgets",
       "_model_module_version": "~2.1.4",
       "_view_module": "jupyter-js-widgets",
       "_view_module_version": "~2.1.4",
       "layout": "IPY_MODEL_dcc6e706789f4e8a80e2390cec290c36",
       "msg_throttle": 1
      }
     },
     "50d71f1cd4174a10a8ceb64eccb21120": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [
        "widget-interact"
       ],
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "children": [
        "IPY_MODEL_c3d4519474934655bb16d61d797a6ffb",
        "IPY_MODEL_3658ab89cef348159fc37268c83286c7"
       ],
       "layout": "IPY_MODEL_6e71d4602bf040158cda58fd4a11a4a6"
      }
     },
     "66481c25e80a4caf9a34e5480c1596f4": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ProgressModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "bar_style": "success",
       "layout": "IPY_MODEL_d8139d687d504e1098e373f239bdef87",
       "max": 1,
       "style": "IPY_MODEL_c68dce6f590142598e9f936a22adc347",
       "value": 1
      }
     },
     "6e71d4602bf040158cda58fd4a11a4a6": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "709320e826654c03a3d478288aa9b5ad": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "89d80e9a948a4ed485767e59fef08005": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "b7bd4dc1f45447d7b9696102aaec4752": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "HBoxModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "children": [
        "IPY_MODEL_66481c25e80a4caf9a34e5480c1596f4",
        "IPY_MODEL_0657313987a047cf9a4b06a7f7562c37"
       ],
       "layout": "IPY_MODEL_709320e826654c03a3d478288aa9b5ad"
      }
     },
     "c3d4519474934655bb16d61d797a6ffb": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "IntTextModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "description": "Pick an Issue No.:",
       "layout": "IPY_MODEL_89d80e9a948a4ed485767e59fef08005",
       "max": 3437,
       "min": 0,
       "step": 1,
       "value": 3
      }
     },
     "c5428b7259c44b02853aa783c60a56ef": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "c68dce6f590142598e9f936a22adc347": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "d8139d687d504e1098e373f239bdef87": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "dcc6e706789f4e8a80e2390cec290c36": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     }
    },
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesizing Data \n",
    "## By: Lela Bones\n",
    "-----------------------------------\n",
    "### Problem\n",
    "    Nestor is an application that allows users to datify their maintenance logs. Datify is the process of taking unorginized data and putting it into a quantifiable and statistically relevant format. The problem is that because of copywright and data ownership technicalities, it is hard for us to have a large amount of \"good\" data to demo our app on. \n",
    "\n",
    "### Solution\n",
    "    I plan on using Reccurent Neural Nets (RNNs) to generate realistic data from the data that we already have from companies. The reason for this is because we can use the \"synthetic\" data to demo our app on, and we aren't breaking any laws. I plan on using a Python library, Pytorch to implement my RNN and train it on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Pytorch???\n",
    "Pytorch is a framework that builds from the Torch framework that Facebook actively uses. Pytorch is extremely fast it is very native and customizable. Pytorch is also a dynamic deep learning tool, which means that you can change and execute notes as you're learning. This makes RNNs way easier to train because you don't need to set a maximum length and then pad smaller sequences. Debugging is really easy because it is defined at runtime. It works well with Flask, which is the tool that I am using to create my visual dashboard. Pytorch also has declarative data parallelism which allows you to use multiple GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why RNNs???\n",
    "    * Vanilla Neural Networks relearn each iteration\n",
    "    * Long Short Term Memory Neural Nets are good for long-term data\n",
    "    * Maybe GRU Neural Nets, they get rid of the disappearing gradient problem\n",
    "    * RNNs implement loops so the learning is compositional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pandas\n",
    "#conda install pytorch torchvision -c pytorch\n",
    "#conda install numpy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set data file, column you want extracted, and output file\n",
    "csv_file = 'mine_raw.csv'\n",
    "txt_file = 'train.txt'\n",
    "text_col = 'OriginalShorttext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function converts the column of text from your csv file \n",
    "#to a text file that has each row on a seperate line\n",
    "#code modified from https://stackoverflow.com/questions/47339698/how-to-convert-csv-file-to-text-file-using-python\n",
    "def createTextFile(inputFile, text, outputFile):\n",
    "    df = pd.read_csv(inputFile)\n",
    "    data = df[text].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "    text_list = []\n",
    "    for line in data:\n",
    "        text_list.append(\" \".join(line))\n",
    "    \n",
    "    #switch to a+ if you want to append to existing file\n",
    "    with open(outputFile, \"w+\") as output_file:\n",
    "        for line in data:\n",
    "            output_file.write(\"  \" + line + \"\\n\")\n",
    "        #verification that it's finished\n",
    "        print('File Successfully written.')\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Successfully written.\n"
     ]
    }
   ],
   "source": [
    "createTextFile(csv_file, text_col, txt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Attempt\n",
    "The majority of this code was modified from https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 165817\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = unidecode.unidecode(open(txt_file).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "#splitting up the code into random chunks each the size of 200\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "# print(random_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 10,  11,  12,  39,  40,  41])\n"
     ]
    }
   ],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor)\n",
    "\n",
    "print(char_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c].unsqueeze(0))\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / chunk_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msid/ltb3/anaconda3/envs/nestorDev/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 37s (100 5%) 1.8912]\n",
      "Wh6KR6h6HLB\n",
      "  rick trotstho\n",
      "  don leakin ol tup stir takt alg bo r/h oling\n",
      "  wir ang sil wrepam moan n \n",
      "\n",
      "[1m 19s (200 10%) 2.0013]\n",
      "Whss on woth shyd 24104\n",
      "  replace hose line hose out\n",
      "  replh r/h prepace line l/h  hyd liling l/h buck \n",
      "\n",
      "[1m 56s (300 15%) 1.4132]\n",
      "Whand motoat switexs\n",
      "  harmids leak ~ rh engine pump\n",
      "  replace ~ talank leaking leake)\n",
      "  replace chang \n",
      "\n",
      "[2m 35s (400 20%) 1.1199]\n",
      "Whk mairs\n",
      "  repair repair bucket start toom hfidys\n",
      "  al bext oils not working\n",
      "  reasre looth over toor \n",
      "\n",
      "[3m 18s (500 25%) 1.1332]\n",
      "Wh hand ring ousing aproken eal shd24\n",
      "  replace boom alant\n",
      "  replace arem fileget filtern pressins\n",
      "  r \n",
      "\n",
      "[4m 1s (600 30%) 1.0488]\n",
      "Wh slew bot on bolts oulter start under\n",
      "  cramp broken grease shd 4 replace coor 4\n",
      "  replace for ganer \n",
      "\n",
      "[4m 44s (700 35%) 1.4145]\n",
      "Wh bucket top\n",
      "  rh alarr shd24.\n",
      "  repair and shd0024\n",
      "  repair bucket tooth\n",
      "  repair aday grease\n",
      "  repa \n",
      "\n",
      "[5m 19s (800 40%) 1.2525]\n",
      "Wh fire rimessor wofff pump senters\n",
      "  replace block in creplaces splate ring\n",
      "  water hissigar\n",
      "  mater  \n",
      "\n",
      "[5m 59s (900 45%) 1.4187]\n",
      "Wh\n",
      "  grease calator shd24\n",
      "  rack filter hose\n",
      "  changeout riveal oil steelt\n",
      "  rh bucket tooth\n",
      "  repair  \n",
      "\n",
      "[6m 40s (1000 50%) 0.8700]\n",
      "Wh slew moruit thes on motork\n",
      "  broken blowigle tooth\n",
      "  damage creas leaking\n",
      "  lh engine warricket\n",
      "  r \n",
      "\n",
      "[7m 25s (1100 55%) 1.2636]\n",
      "Wh air changeout ~ stick top spauls\n",
      "  rf uist cooling ver valve\n",
      "  pump hydraulic oil comming leak (hos \n",
      "\n",
      "[8m 5s (1200 60%) 1.3749]\n",
      "Wh-wayping on bude filters\n",
      "  repair gaining down oil leak\n",
      "  side oil leak\n",
      "  repair slew ind fault d/o  \n",
      "\n",
      "[8m 42s (1300 65%) 1.0517]\n",
      "Wh slew hose\n",
      "  oil leak and clammines.oing\n",
      "  replace grease line on bolt r/h slew motor hose blown hos \n",
      "\n",
      "[9m 20s (1400 70%) 0.7619]\n",
      "Whly #01 dampstick\n",
      "  replace grease lines on boom block\n",
      "  repair car fitter changeout.\n",
      "  cleane light  \n",
      "\n",
      "[9m 57s (1500 75%) 1.4143]\n",
      "Wh fault piresinovjo\n",
      "  hyd oin shd24  rh maunt shd24\n",
      "  replace lh shd24\n",
      "  slew tramps damage depear sh \n",
      "\n",
      "[10m 44s (1600 80%) 0.9683]\n",
      "Wh bucket leak\n",
      "  replace blown loble lhs slew boom box on brake\n",
      "  grease leak l/h engine wons engine t \n",
      "\n",
      "[11m 24s (1700 85%) 1.1574]\n",
      "Wh indarter hoses after cleaning\n",
      "  repair haunting on complecting on.\n",
      "  replace lh leak idler hose doo \n",
      "\n",
      "[12m 6s (1800 90%) 1.1625]\n",
      "Wh box conditioner gald dank\n",
      "  oil blocking a/c compretooth.\n",
      "  replace repair engine alarm fan @16 bot \n",
      "\n",
      "[12m 51s (1900 95%) 1.5989]\n",
      "Wh a/c\n",
      "  replace fit tip\n",
      "  rh travel hot histor\n",
      "  lines on slew motor repair pin\n",
      "  replace lidsing oil \n",
      "\n",
      "[13m 34s (2000 100%) 1.6275]\n",
      "Wh neection in strack\n",
      "  replace alarm\n",
      "  right screec fac\n",
      "  lube not woper motor\n",
      "  replace switch cylin \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "lr = 0.005\n",
    "\n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set())       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f999a764a20>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Attempt\n",
    "Most of this code is from https://github.com/mcleonard/pytorch-charRNN/blob/master/TorchRNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "with open (txt_file, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert char to int\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns mini-batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y\n",
    "\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network '''\n",
    "        \n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs\n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        inputs = Variable(torch.from_numpy(x), volatile=True)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([Variable(each.data, volatile=True) for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()),\n",
    "                Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    ''' Traing a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            inputs, targets = Variable(x), Variable(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([Variable(each.data, volatile=True) for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = Variable(x, volatile=True), Variable(y, volatile=True)\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.data[0])\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.data[0]),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltb3/.conda/envs/mwo-lstm/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/ltb3/.conda/envs/mwo-lstm/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/ltb3/.conda/envs/mwo-lstm/lib/python3.6/site-packages/ipykernel_launcher.py:76: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/ltb3/.conda/envs/mwo-lstm/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/ltb3/.conda/envs/mwo-lstm/lib/python3.6/site-packages/ipykernel_launcher.py:87: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25... Step: 10... Loss: 3.1611... Val Loss: 3.1850\n",
      "Epoch: 1/25... Step: 20... Loss: 3.0247... Val Loss: 3.0601\n",
      "Epoch: 1/25... Step: 30... Loss: 2.8449... Val Loss: 2.9841\n",
      "Epoch: 1/25... Step: 40... Loss: 2.7697... Val Loss: 2.9044\n",
      "Epoch: 1/25... Step: 50... Loss: 2.7963... Val Loss: 2.8121\n",
      "Epoch: 1/25... Step: 60... Loss: 2.5574... Val Loss: 2.7220\n",
      "Epoch: 1/25... Step: 70... Loss: 2.3822... Val Loss: 2.6780\n",
      "Epoch: 1/25... Step: 80... Loss: 2.2556... Val Loss: 2.6236\n",
      "Epoch: 1/25... Step: 90... Loss: 2.2579... Val Loss: 2.6238\n",
      "Epoch: 1/25... Step: 100... Loss: 2.2720... Val Loss: 2.5818\n",
      "Epoch: 1/25... Step: 110... Loss: 2.2795... Val Loss: 2.5855\n",
      "Epoch: 1/25... Step: 120... Loss: 2.2292... Val Loss: 2.5649\n",
      "Epoch: 1/25... Step: 130... Loss: 1.9819... Val Loss: 2.5388\n",
      "Epoch: 1/25... Step: 140... Loss: 1.8547... Val Loss: 2.5047\n",
      "Epoch: 1/25... Step: 150... Loss: 2.0360... Val Loss: 2.5264\n",
      "Epoch: 1/25... Step: 160... Loss: 1.9774... Val Loss: 2.5154\n",
      "Epoch: 1/25... Step: 170... Loss: 1.9593... Val Loss: 2.4684\n",
      "Epoch: 1/25... Step: 180... Loss: 1.9875... Val Loss: 2.4817\n",
      "Epoch: 1/25... Step: 190... Loss: 1.7551... Val Loss: 2.4214\n",
      "Epoch: 1/25... Step: 200... Loss: 1.6858... Val Loss: 2.4357\n",
      "Epoch: 1/25... Step: 210... Loss: 2.0619... Val Loss: 2.3921\n",
      "Epoch: 1/25... Step: 220... Loss: 1.9572... Val Loss: 2.3520\n",
      "Epoch: 1/25... Step: 230... Loss: 1.7862... Val Loss: 2.3054\n",
      "Epoch: 1/25... Step: 240... Loss: 1.8130... Val Loss: 2.3146\n",
      "Epoch: 1/25... Step: 250... Loss: 1.7072... Val Loss: 2.3051\n",
      "Epoch: 1/25... Step: 260... Loss: 1.9566... Val Loss: 2.3095\n",
      "Epoch: 1/25... Step: 270... Loss: 1.8058... Val Loss: 2.2930\n",
      "Epoch: 1/25... Step: 280... Loss: 1.9436... Val Loss: 2.2673\n",
      "Epoch: 1/25... Step: 290... Loss: 1.8688... Val Loss: 2.2698\n",
      "Epoch: 2/25... Step: 300... Loss: 1.6072... Val Loss: 2.2701\n",
      "Epoch: 2/25... Step: 310... Loss: 1.6656... Val Loss: 2.2771\n",
      "Epoch: 2/25... Step: 320... Loss: 1.7975... Val Loss: 2.2937\n",
      "Epoch: 2/25... Step: 330... Loss: 1.7145... Val Loss: 2.2191\n",
      "Epoch: 2/25... Step: 340... Loss: 1.4848... Val Loss: 2.2540\n",
      "Epoch: 2/25... Step: 350... Loss: 1.6309... Val Loss: 2.2369\n",
      "Epoch: 2/25... Step: 360... Loss: 1.7295... Val Loss: 2.2175\n",
      "Epoch: 2/25... Step: 370... Loss: 1.6907... Val Loss: 2.1755\n",
      "Epoch: 2/25... Step: 380... Loss: 1.5390... Val Loss: 2.1667\n",
      "Epoch: 2/25... Step: 390... Loss: 1.4563... Val Loss: 2.1677\n",
      "Epoch: 2/25... Step: 400... Loss: 1.6518... Val Loss: 2.1185\n",
      "Epoch: 2/25... Step: 410... Loss: 1.8252... Val Loss: 2.1398\n",
      "Epoch: 2/25... Step: 420... Loss: 1.6343... Val Loss: 2.1142\n",
      "Epoch: 2/25... Step: 430... Loss: 1.7751... Val Loss: 2.1243\n",
      "Epoch: 2/25... Step: 440... Loss: 1.6298... Val Loss: 2.0991\n",
      "Epoch: 2/25... Step: 450... Loss: 1.5133... Val Loss: 2.1208\n",
      "Epoch: 2/25... Step: 460... Loss: 1.5057... Val Loss: 2.1485\n",
      "Epoch: 2/25... Step: 470... Loss: 1.3900... Val Loss: 2.0975\n",
      "Epoch: 2/25... Step: 480... Loss: 1.6565... Val Loss: 2.0950\n",
      "Epoch: 2/25... Step: 490... Loss: 1.3391... Val Loss: 2.0976\n",
      "Epoch: 2/25... Step: 500... Loss: 1.5983... Val Loss: 2.0613\n",
      "Epoch: 2/25... Step: 510... Loss: 1.3377... Val Loss: 2.0571\n",
      "Epoch: 2/25... Step: 520... Loss: 1.4925... Val Loss: 2.0506\n",
      "Epoch: 2/25... Step: 530... Loss: 1.5329... Val Loss: 1.9908\n",
      "Epoch: 2/25... Step: 540... Loss: 1.4294... Val Loss: 2.0358\n",
      "Epoch: 2/25... Step: 550... Loss: 1.7100... Val Loss: 2.0246\n",
      "Epoch: 2/25... Step: 560... Loss: 1.4871... Val Loss: 2.0373\n",
      "Epoch: 2/25... Step: 570... Loss: 1.5656... Val Loss: 2.0047\n",
      "Epoch: 2/25... Step: 580... Loss: 1.3974... Val Loss: 1.9879\n",
      "Epoch: 2/25... Step: 590... Loss: 1.5680... Val Loss: 2.0040\n",
      "Epoch: 3/25... Step: 600... Loss: 1.3296... Val Loss: 2.0092\n",
      "Epoch: 3/25... Step: 610... Loss: 1.2678... Val Loss: 2.0002\n",
      "Epoch: 3/25... Step: 620... Loss: 1.3688... Val Loss: 2.0262\n",
      "Epoch: 3/25... Step: 630... Loss: 1.4522... Val Loss: 1.9786\n",
      "Epoch: 3/25... Step: 640... Loss: 1.3859... Val Loss: 2.0557\n",
      "Epoch: 3/25... Step: 650... Loss: 1.5299... Val Loss: 1.9918\n",
      "Epoch: 3/25... Step: 660... Loss: 1.3191... Val Loss: 2.0074\n",
      "Epoch: 3/25... Step: 670... Loss: 1.6003... Val Loss: 1.9843\n",
      "Epoch: 3/25... Step: 680... Loss: 1.3933... Val Loss: 2.0105\n",
      "Epoch: 3/25... Step: 690... Loss: 1.2159... Val Loss: 1.9557\n",
      "Epoch: 3/25... Step: 700... Loss: 1.4666... Val Loss: 1.9294\n",
      "Epoch: 3/25... Step: 710... Loss: 1.4114... Val Loss: 1.9720\n",
      "Epoch: 3/25... Step: 720... Loss: 1.4085... Val Loss: 1.9614\n",
      "Epoch: 3/25... Step: 730... Loss: 1.4052... Val Loss: 1.9640\n",
      "Epoch: 3/25... Step: 740... Loss: 1.2999... Val Loss: 1.9458\n",
      "Epoch: 3/25... Step: 750... Loss: 1.6166... Val Loss: 1.9710\n",
      "Epoch: 3/25... Step: 760... Loss: 1.2814... Val Loss: 1.9752\n",
      "Epoch: 3/25... Step: 770... Loss: 1.0391... Val Loss: 1.9497\n",
      "Epoch: 3/25... Step: 780... Loss: 1.4388... Val Loss: 1.9471\n",
      "Epoch: 3/25... Step: 790... Loss: 1.3523... Val Loss: 1.9529\n",
      "Epoch: 3/25... Step: 800... Loss: 1.2908... Val Loss: 1.9198\n",
      "Epoch: 3/25... Step: 810... Loss: 1.4029... Val Loss: 1.9520\n",
      "Epoch: 3/25... Step: 820... Loss: 1.2139... Val Loss: 1.9167\n",
      "Epoch: 3/25... Step: 830... Loss: 1.6416... Val Loss: 1.8740\n",
      "Epoch: 3/25... Step: 840... Loss: 1.2601... Val Loss: 1.9043\n",
      "Epoch: 3/25... Step: 850... Loss: 1.3083... Val Loss: 1.9057\n",
      "Epoch: 3/25... Step: 860... Loss: 1.3740... Val Loss: 1.9193\n",
      "Epoch: 3/25... Step: 870... Loss: 1.2220... Val Loss: 1.8907\n",
      "Epoch: 3/25... Step: 880... Loss: 1.4290... Val Loss: 1.8982\n",
      "Epoch: 3/25... Step: 890... Loss: 1.3038... Val Loss: 1.9016\n",
      "Epoch: 4/25... Step: 900... Loss: 1.2915... Val Loss: 1.9094\n",
      "Epoch: 4/25... Step: 910... Loss: 1.6716... Val Loss: 1.8984\n",
      "Epoch: 4/25... Step: 920... Loss: 1.4964... Val Loss: 1.8909\n",
      "Epoch: 4/25... Step: 930... Loss: 1.2790... Val Loss: 1.8952\n",
      "Epoch: 4/25... Step: 940... Loss: 1.1500... Val Loss: 1.9119\n",
      "Epoch: 4/25... Step: 950... Loss: 1.1896... Val Loss: 1.8866\n",
      "Epoch: 4/25... Step: 960... Loss: 1.4130... Val Loss: 1.8814\n",
      "Epoch: 4/25... Step: 970... Loss: 1.2916... Val Loss: 1.8828\n",
      "Epoch: 4/25... Step: 980... Loss: 1.3314... Val Loss: 1.9067\n",
      "Epoch: 4/25... Step: 990... Loss: 1.3404... Val Loss: 1.8751\n",
      "Epoch: 4/25... Step: 1000... Loss: 1.5158... Val Loss: 1.8492\n",
      "Epoch: 4/25... Step: 1010... Loss: 1.2447... Val Loss: 1.8774\n",
      "Epoch: 4/25... Step: 1020... Loss: 1.4671... Val Loss: 1.8747\n",
      "Epoch: 4/25... Step: 1030... Loss: 1.1779... Val Loss: 1.8523\n",
      "Epoch: 4/25... Step: 1040... Loss: 1.1878... Val Loss: 1.8473\n",
      "Epoch: 4/25... Step: 1050... Loss: 1.1288... Val Loss: 1.8828\n",
      "Epoch: 4/25... Step: 1060... Loss: 1.0971... Val Loss: 1.9179\n",
      "Epoch: 4/25... Step: 1070... Loss: 1.2825... Val Loss: 1.9092\n",
      "Epoch: 4/25... Step: 1080... Loss: 1.2464... Val Loss: 1.8695\n",
      "Epoch: 4/25... Step: 1090... Loss: 1.1285... Val Loss: 1.8489\n",
      "Epoch: 4/25... Step: 1100... Loss: 1.1364... Val Loss: 1.8341\n",
      "Epoch: 4/25... Step: 1110... Loss: 1.1950... Val Loss: 1.8375\n",
      "Epoch: 4/25... Step: 1120... Loss: 1.3249... Val Loss: 1.8343\n",
      "Epoch: 4/25... Step: 1130... Loss: 1.2891... Val Loss: 1.8095\n",
      "Epoch: 4/25... Step: 1140... Loss: 1.2428... Val Loss: 1.8418\n",
      "Epoch: 4/25... Step: 1150... Loss: 1.1199... Val Loss: 1.8278\n",
      "Epoch: 4/25... Step: 1160... Loss: 1.1252... Val Loss: 1.8176\n",
      "Epoch: 4/25... Step: 1170... Loss: 1.3297... Val Loss: 1.8391\n",
      "Epoch: 4/25... Step: 1180... Loss: 1.5061... Val Loss: 1.8386\n",
      "Epoch: 4/25... Step: 1190... Loss: 1.4392... Val Loss: 1.7923\n",
      "Epoch: 5/25... Step: 1200... Loss: 1.0468... Val Loss: 1.8609\n",
      "Epoch: 5/25... Step: 1210... Loss: 1.2517... Val Loss: 1.8070\n",
      "Epoch: 5/25... Step: 1220... Loss: 1.1833... Val Loss: 1.8289\n",
      "Epoch: 5/25... Step: 1230... Loss: 1.0284... Val Loss: 1.8131\n",
      "Epoch: 5/25... Step: 1240... Loss: 1.1635... Val Loss: 1.8559\n",
      "Epoch: 5/25... Step: 1250... Loss: 1.0740... Val Loss: 1.7970\n",
      "Epoch: 5/25... Step: 1260... Loss: 1.0744... Val Loss: 1.8443\n",
      "Epoch: 5/25... Step: 1270... Loss: 1.1690... Val Loss: 1.8104\n",
      "Epoch: 5/25... Step: 1280... Loss: 1.0688... Val Loss: 1.8436\n",
      "Epoch: 5/25... Step: 1290... Loss: 1.1365... Val Loss: 1.8106\n",
      "Epoch: 5/25... Step: 1300... Loss: 1.3216... Val Loss: 1.8106\n",
      "Epoch: 5/25... Step: 1310... Loss: 1.1761... Val Loss: 1.8180\n",
      "Epoch: 5/25... Step: 1320... Loss: 1.1911... Val Loss: 1.8086\n",
      "Epoch: 5/25... Step: 1330... Loss: 1.1994... Val Loss: 1.8029\n",
      "Epoch: 5/25... Step: 1340... Loss: 1.0769... Val Loss: 1.8080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/25... Step: 1350... Loss: 1.1337... Val Loss: 1.8069\n",
      "Epoch: 5/25... Step: 1360... Loss: 1.1393... Val Loss: 1.8584\n",
      "Epoch: 5/25... Step: 1370... Loss: 1.1240... Val Loss: 1.8296\n",
      "Epoch: 5/25... Step: 1380... Loss: 1.0045... Val Loss: 1.8408\n",
      "Epoch: 5/25... Step: 1390... Loss: 1.2977... Val Loss: 1.8220\n",
      "Epoch: 5/25... Step: 1400... Loss: 1.0608... Val Loss: 1.7783\n",
      "Epoch: 5/25... Step: 1410... Loss: 1.0186... Val Loss: 1.7783\n",
      "Epoch: 5/25... Step: 1420... Loss: 1.2059... Val Loss: 1.7692\n",
      "Epoch: 5/25... Step: 1430... Loss: 1.1771... Val Loss: 1.7754\n",
      "Epoch: 5/25... Step: 1440... Loss: 1.0417... Val Loss: 1.7918\n",
      "Epoch: 5/25... Step: 1450... Loss: 1.2134... Val Loss: 1.7861\n",
      "Epoch: 5/25... Step: 1460... Loss: 0.9494... Val Loss: 1.7721\n",
      "Epoch: 5/25... Step: 1470... Loss: 1.1761... Val Loss: 1.7871\n",
      "Epoch: 5/25... Step: 1480... Loss: 1.0474... Val Loss: 1.7894\n",
      "Epoch: 5/25... Step: 1490... Loss: 1.1727... Val Loss: 1.7464\n",
      "Epoch: 6/25... Step: 1500... Loss: 1.0800... Val Loss: 1.8085\n",
      "Epoch: 6/25... Step: 1510... Loss: 0.9978... Val Loss: 1.7534\n",
      "Epoch: 6/25... Step: 1520... Loss: 0.9987... Val Loss: 1.7894\n",
      "Epoch: 6/25... Step: 1530... Loss: 0.9932... Val Loss: 1.7890\n",
      "Epoch: 6/25... Step: 1540... Loss: 1.2058... Val Loss: 1.8116\n",
      "Epoch: 6/25... Step: 1550... Loss: 1.0992... Val Loss: 1.7479\n",
      "Epoch: 6/25... Step: 1560... Loss: 0.8653... Val Loss: 1.8077\n",
      "Epoch: 6/25... Step: 1570... Loss: 0.9637... Val Loss: 1.7651\n",
      "Epoch: 6/25... Step: 1580... Loss: 1.0861... Val Loss: 1.8297\n",
      "Epoch: 6/25... Step: 1590... Loss: 1.2067... Val Loss: 1.7667\n",
      "Epoch: 6/25... Step: 1600... Loss: 1.1844... Val Loss: 1.7840\n",
      "Epoch: 6/25... Step: 1610... Loss: 1.1006... Val Loss: 1.8166\n",
      "Epoch: 6/25... Step: 1620... Loss: 0.9479... Val Loss: 1.7639\n",
      "Epoch: 6/25... Step: 1630... Loss: 1.0724... Val Loss: 1.7821\n",
      "Epoch: 6/25... Step: 1640... Loss: 0.9120... Val Loss: 1.7984\n",
      "Epoch: 6/25... Step: 1650... Loss: 0.9073... Val Loss: 1.7994\n",
      "Epoch: 6/25... Step: 1660... Loss: 0.9830... Val Loss: 1.7966\n",
      "Epoch: 6/25... Step: 1670... Loss: 1.2868... Val Loss: 1.7875\n",
      "Epoch: 6/25... Step: 1680... Loss: 0.9095... Val Loss: 1.7767\n",
      "Epoch: 6/25... Step: 1690... Loss: 0.9758... Val Loss: 1.7780\n",
      "Epoch: 6/25... Step: 1700... Loss: 1.2379... Val Loss: 1.7241\n",
      "Epoch: 6/25... Step: 1710... Loss: 1.1871... Val Loss: 1.7353\n",
      "Epoch: 6/25... Step: 1720... Loss: 1.1685... Val Loss: 1.7396\n",
      "Epoch: 6/25... Step: 1730... Loss: 1.0311... Val Loss: 1.7464\n",
      "Epoch: 6/25... Step: 1740... Loss: 0.9466... Val Loss: 1.7510\n",
      "Epoch: 6/25... Step: 1750... Loss: 1.2675... Val Loss: 1.7685\n",
      "Epoch: 6/25... Step: 1760... Loss: 1.1040... Val Loss: 1.7323\n",
      "Epoch: 6/25... Step: 1770... Loss: 1.0088... Val Loss: 1.7572\n",
      "Epoch: 6/25... Step: 1780... Loss: 1.1403... Val Loss: 1.7668\n",
      "Epoch: 7/25... Step: 1790... Loss: 0.9944... Val Loss: 1.7585\n",
      "Epoch: 7/25... Step: 1800... Loss: 0.9886... Val Loss: 1.7990\n",
      "Epoch: 7/25... Step: 1810... Loss: 1.0517... Val Loss: 1.7317\n",
      "Epoch: 7/25... Step: 1820... Loss: 1.1345... Val Loss: 1.7689\n",
      "Epoch: 7/25... Step: 1830... Loss: 0.9065... Val Loss: 1.7757\n",
      "Epoch: 7/25... Step: 1840... Loss: 1.0366... Val Loss: 1.7618\n",
      "Epoch: 7/25... Step: 1850... Loss: 1.1701... Val Loss: 1.7303\n",
      "Epoch: 7/25... Step: 1860... Loss: 1.1435... Val Loss: 1.7783\n",
      "Epoch: 7/25... Step: 1870... Loss: 0.9342... Val Loss: 1.7608\n",
      "Epoch: 7/25... Step: 1880... Loss: 0.9777... Val Loss: 1.7807\n",
      "Epoch: 7/25... Step: 1890... Loss: 1.0165... Val Loss: 1.7585\n",
      "Epoch: 7/25... Step: 1900... Loss: 1.1222... Val Loss: 1.7770\n",
      "Epoch: 7/25... Step: 1910... Loss: 1.0984... Val Loss: 1.7905\n",
      "Epoch: 7/25... Step: 1920... Loss: 1.1110... Val Loss: 1.7519\n",
      "Epoch: 7/25... Step: 1930... Loss: 1.0263... Val Loss: 1.7735\n",
      "Epoch: 7/25... Step: 1940... Loss: 1.1087... Val Loss: 1.7924\n",
      "Epoch: 7/25... Step: 1950... Loss: 0.9073... Val Loss: 1.7943\n",
      "Epoch: 7/25... Step: 1960... Loss: 0.9076... Val Loss: 1.8146\n",
      "Epoch: 7/25... Step: 1970... Loss: 1.0867... Val Loss: 1.7461\n",
      "Epoch: 7/25... Step: 1980... Loss: 0.9091... Val Loss: 1.7629\n",
      "Epoch: 7/25... Step: 1990... Loss: 1.1350... Val Loss: 1.7299\n",
      "Epoch: 7/25... Step: 2000... Loss: 0.9051... Val Loss: 1.6879\n",
      "Epoch: 7/25... Step: 2010... Loss: 1.0422... Val Loss: 1.7130\n",
      "Epoch: 7/25... Step: 2020... Loss: 1.0322... Val Loss: 1.6791\n",
      "Epoch: 7/25... Step: 2030... Loss: 0.8811... Val Loss: 1.7238\n",
      "Epoch: 7/25... Step: 2040... Loss: 1.2360... Val Loss: 1.7350\n",
      "Epoch: 7/25... Step: 2050... Loss: 1.0373... Val Loss: 1.7267\n",
      "Epoch: 7/25... Step: 2060... Loss: 1.0735... Val Loss: 1.7145\n",
      "Epoch: 7/25... Step: 2070... Loss: 0.9917... Val Loss: 1.7054\n",
      "Epoch: 7/25... Step: 2080... Loss: 1.0421... Val Loss: 1.7033\n",
      "Epoch: 8/25... Step: 2090... Loss: 0.8981... Val Loss: 1.7143\n",
      "Epoch: 8/25... Step: 2100... Loss: 0.8123... Val Loss: 1.7669\n",
      "Epoch: 8/25... Step: 2110... Loss: 0.9782... Val Loss: 1.7190\n",
      "Epoch: 8/25... Step: 2120... Loss: 0.9437... Val Loss: 1.7108\n",
      "Epoch: 8/25... Step: 2130... Loss: 0.9375... Val Loss: 1.7443\n",
      "Epoch: 8/25... Step: 2140... Loss: 1.0447... Val Loss: 1.7093\n",
      "Epoch: 8/25... Step: 2150... Loss: 0.9340... Val Loss: 1.7223\n",
      "Epoch: 8/25... Step: 2160... Loss: 1.2453... Val Loss: 1.7371\n",
      "Epoch: 8/25... Step: 2170... Loss: 0.9596... Val Loss: 1.7313\n",
      "Epoch: 8/25... Step: 2180... Loss: 0.9247... Val Loss: 1.7606\n",
      "Epoch: 8/25... Step: 2190... Loss: 0.9830... Val Loss: 1.7119\n",
      "Epoch: 8/25... Step: 2200... Loss: 1.0284... Val Loss: 1.7522\n",
      "Epoch: 8/25... Step: 2210... Loss: 1.0285... Val Loss: 1.7374\n",
      "Epoch: 8/25... Step: 2220... Loss: 1.1081... Val Loss: 1.7058\n",
      "Epoch: 8/25... Step: 2230... Loss: 0.9480... Val Loss: 1.7613\n",
      "Epoch: 8/25... Step: 2240... Loss: 1.1198... Val Loss: 1.7557\n",
      "Epoch: 8/25... Step: 2250... Loss: 0.9224... Val Loss: 1.7695\n",
      "Epoch: 8/25... Step: 2260... Loss: 0.7620... Val Loss: 1.7462\n",
      "Epoch: 8/25... Step: 2270... Loss: 1.0392... Val Loss: 1.7189\n",
      "Epoch: 8/25... Step: 2280... Loss: 1.0549... Val Loss: 1.7190\n",
      "Epoch: 8/25... Step: 2290... Loss: 0.9254... Val Loss: 1.7362\n",
      "Epoch: 8/25... Step: 2300... Loss: 1.0376... Val Loss: 1.6869\n",
      "Epoch: 8/25... Step: 2310... Loss: 0.9348... Val Loss: 1.6906\n",
      "Epoch: 8/25... Step: 2320... Loss: 1.1989... Val Loss: 1.6594\n",
      "Epoch: 8/25... Step: 2330... Loss: 0.8865... Val Loss: 1.7085\n",
      "Epoch: 8/25... Step: 2340... Loss: 0.9283... Val Loss: 1.7214\n",
      "Epoch: 8/25... Step: 2350... Loss: 1.0147... Val Loss: 1.7341\n",
      "Epoch: 8/25... Step: 2360... Loss: 0.8834... Val Loss: 1.6961\n",
      "Epoch: 8/25... Step: 2370... Loss: 1.0703... Val Loss: 1.7109\n",
      "Epoch: 8/25... Step: 2380... Loss: 0.9897... Val Loss: 1.6750\n",
      "Epoch: 9/25... Step: 2390... Loss: 0.9773... Val Loss: 1.7200\n",
      "Epoch: 9/25... Step: 2400... Loss: 1.2895... Val Loss: 1.7166\n",
      "Epoch: 9/25... Step: 2410... Loss: 1.1480... Val Loss: 1.6942\n",
      "Epoch: 9/25... Step: 2420... Loss: 0.9428... Val Loss: 1.6888\n",
      "Epoch: 9/25... Step: 2430... Loss: 0.8359... Val Loss: 1.7318\n",
      "Epoch: 9/25... Step: 2440... Loss: 0.9908... Val Loss: 1.6862\n",
      "Epoch: 9/25... Step: 2450... Loss: 1.0451... Val Loss: 1.7078\n",
      "Epoch: 9/25... Step: 2460... Loss: 1.0347... Val Loss: 1.7119\n",
      "Epoch: 9/25... Step: 2470... Loss: 0.9558... Val Loss: 1.7222\n",
      "Epoch: 9/25... Step: 2480... Loss: 0.9763... Val Loss: 1.7376\n",
      "Epoch: 9/25... Step: 2490... Loss: 1.2131... Val Loss: 1.6979\n",
      "Epoch: 9/25... Step: 2500... Loss: 0.9431... Val Loss: 1.7382\n",
      "Epoch: 9/25... Step: 2510... Loss: 1.1053... Val Loss: 1.7486\n",
      "Epoch: 9/25... Step: 2520... Loss: 0.9181... Val Loss: 1.6977\n",
      "Epoch: 9/25... Step: 2530... Loss: 0.8747... Val Loss: 1.7032\n",
      "Epoch: 9/25... Step: 2540... Loss: 0.8659... Val Loss: 1.7421\n",
      "Epoch: 9/25... Step: 2550... Loss: 0.8553... Val Loss: 1.7500\n",
      "Epoch: 9/25... Step: 2560... Loss: 0.9300... Val Loss: 1.7431\n",
      "Epoch: 9/25... Step: 2570... Loss: 1.0150... Val Loss: 1.7093\n",
      "Epoch: 9/25... Step: 2580... Loss: 0.8789... Val Loss: 1.7290\n",
      "Epoch: 9/25... Step: 2590... Loss: 1.0118... Val Loss: 1.7151\n",
      "Epoch: 9/25... Step: 2600... Loss: 0.9750... Val Loss: 1.6766\n",
      "Epoch: 9/25... Step: 2610... Loss: 0.9864... Val Loss: 1.6832\n",
      "Epoch: 9/25... Step: 2620... Loss: 1.0097... Val Loss: 1.6966\n",
      "Epoch: 9/25... Step: 2630... Loss: 1.0201... Val Loss: 1.7154\n",
      "Epoch: 9/25... Step: 2640... Loss: 0.9084... Val Loss: 1.6926\n",
      "Epoch: 9/25... Step: 2650... Loss: 0.9027... Val Loss: 1.7227\n",
      "Epoch: 9/25... Step: 2660... Loss: 1.0308... Val Loss: 1.7151\n",
      "Epoch: 9/25... Step: 2670... Loss: 1.2270... Val Loss: 1.6988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/25... Step: 2680... Loss: 1.1420... Val Loss: 1.7013\n",
      "Epoch: 10/25... Step: 2690... Loss: 0.9009... Val Loss: 1.7194\n",
      "Epoch: 10/25... Step: 2700... Loss: 0.9752... Val Loss: 1.7358\n",
      "Epoch: 10/25... Step: 2710... Loss: 0.9609... Val Loss: 1.6884\n",
      "Epoch: 10/25... Step: 2720... Loss: 0.7515... Val Loss: 1.7004\n",
      "Epoch: 10/25... Step: 2730... Loss: 0.9217... Val Loss: 1.7725\n",
      "Epoch: 10/25... Step: 2740... Loss: 0.8611... Val Loss: 1.6859\n",
      "Epoch: 10/25... Step: 2750... Loss: 0.8408... Val Loss: 1.7114\n",
      "Epoch: 10/25... Step: 2760... Loss: 0.9955... Val Loss: 1.7358\n",
      "Epoch: 10/25... Step: 2770... Loss: 0.8379... Val Loss: 1.7297\n",
      "Epoch: 10/25... Step: 2780... Loss: 0.8658... Val Loss: 1.7221\n",
      "Epoch: 10/25... Step: 2790... Loss: 1.0940... Val Loss: 1.7037\n",
      "Epoch: 10/25... Step: 2800... Loss: 0.9385... Val Loss: 1.7300\n",
      "Epoch: 10/25... Step: 2810... Loss: 0.9872... Val Loss: 1.7241\n",
      "Epoch: 10/25... Step: 2820... Loss: 0.9370... Val Loss: 1.6729\n",
      "Epoch: 10/25... Step: 2830... Loss: 0.9378... Val Loss: 1.7153\n",
      "Epoch: 10/25... Step: 2840... Loss: 0.8581... Val Loss: 1.7383\n",
      "Epoch: 10/25... Step: 2850... Loss: 0.8734... Val Loss: 1.7309\n",
      "Epoch: 10/25... Step: 2860... Loss: 0.8024... Val Loss: 1.7501\n",
      "Epoch: 10/25... Step: 2870... Loss: 0.7985... Val Loss: 1.7049\n",
      "Epoch: 10/25... Step: 2880... Loss: 1.1130... Val Loss: 1.7133\n",
      "Epoch: 10/25... Step: 2890... Loss: 0.9405... Val Loss: 1.7029\n",
      "Epoch: 10/25... Step: 2900... Loss: 0.8820... Val Loss: 1.6711\n",
      "Epoch: 10/25... Step: 2910... Loss: 1.0287... Val Loss: 1.7092\n",
      "Epoch: 10/25... Step: 2920... Loss: 0.9326... Val Loss: 1.6686\n",
      "Epoch: 10/25... Step: 2930... Loss: 0.9066... Val Loss: 1.7161\n",
      "Epoch: 10/25... Step: 2940... Loss: 0.9585... Val Loss: 1.7049\n",
      "Epoch: 10/25... Step: 2950... Loss: 0.7906... Val Loss: 1.6870\n",
      "Epoch: 10/25... Step: 2960... Loss: 0.9956... Val Loss: 1.6833\n",
      "Epoch: 10/25... Step: 2970... Loss: 0.8622... Val Loss: 1.6920\n",
      "Epoch: 10/25... Step: 2980... Loss: 1.0455... Val Loss: 1.6664\n",
      "Epoch: 11/25... Step: 2990... Loss: 0.8341... Val Loss: 1.7280\n",
      "Epoch: 11/25... Step: 3000... Loss: 0.7443... Val Loss: 1.6926\n",
      "Epoch: 11/25... Step: 3010... Loss: 0.8070... Val Loss: 1.6668\n",
      "Epoch: 11/25... Step: 3020... Loss: 0.8711... Val Loss: 1.6802\n",
      "Epoch: 11/25... Step: 3030... Loss: 0.9318... Val Loss: 1.7227\n",
      "Epoch: 11/25... Step: 3040... Loss: 0.8848... Val Loss: 1.6619\n",
      "Epoch: 11/25... Step: 3050... Loss: 0.7752... Val Loss: 1.7043\n",
      "Epoch: 11/25... Step: 3060... Loss: 0.8118... Val Loss: 1.6968\n",
      "Epoch: 11/25... Step: 3070... Loss: 0.8982... Val Loss: 1.7508\n",
      "Epoch: 11/25... Step: 3080... Loss: 0.9671... Val Loss: 1.7160\n",
      "Epoch: 11/25... Step: 3090... Loss: 0.9012... Val Loss: 1.7092\n",
      "Epoch: 11/25... Step: 3100... Loss: 0.8647... Val Loss: 1.7733\n",
      "Epoch: 11/25... Step: 3110... Loss: 0.7754... Val Loss: 1.7223\n",
      "Epoch: 11/25... Step: 3120... Loss: 0.8515... Val Loss: 1.7031\n",
      "Epoch: 11/25... Step: 3130... Loss: 0.7667... Val Loss: 1.7040\n",
      "Epoch: 11/25... Step: 3140... Loss: 0.6868... Val Loss: 1.7324\n",
      "Epoch: 11/25... Step: 3150... Loss: 0.8188... Val Loss: 1.7172\n",
      "Epoch: 11/25... Step: 3160... Loss: 0.9806... Val Loss: 1.7372\n",
      "Epoch: 11/25... Step: 3170... Loss: 0.7721... Val Loss: 1.6828\n",
      "Epoch: 11/25... Step: 3180... Loss: 0.8803... Val Loss: 1.7065\n",
      "Epoch: 11/25... Step: 3190... Loss: 1.0043... Val Loss: 1.6936\n",
      "Epoch: 11/25... Step: 3200... Loss: 0.9876... Val Loss: 1.6873\n",
      "Epoch: 11/25... Step: 3210... Loss: 0.8991... Val Loss: 1.6893\n",
      "Epoch: 11/25... Step: 3220... Loss: 0.8086... Val Loss: 1.6869\n",
      "Epoch: 11/25... Step: 3230... Loss: 0.7712... Val Loss: 1.7027\n",
      "Epoch: 11/25... Step: 3240... Loss: 0.9873... Val Loss: 1.6986\n",
      "Epoch: 11/25... Step: 3250... Loss: 0.8709... Val Loss: 1.7172\n",
      "Epoch: 11/25... Step: 3260... Loss: 0.7871... Val Loss: 1.7004\n",
      "Epoch: 11/25... Step: 3270... Loss: 0.9201... Val Loss: 1.6964\n",
      "Epoch: 12/25... Step: 3280... Loss: 0.8432... Val Loss: 1.6929\n",
      "Epoch: 12/25... Step: 3290... Loss: 0.8383... Val Loss: 1.7192\n",
      "Epoch: 12/25... Step: 3300... Loss: 0.8797... Val Loss: 1.7176\n",
      "Epoch: 12/25... Step: 3310... Loss: 0.9302... Val Loss: 1.7207\n",
      "Epoch: 12/25... Step: 3320... Loss: 0.7328... Val Loss: 1.7230\n",
      "Epoch: 12/25... Step: 3330... Loss: 0.9293... Val Loss: 1.7864\n",
      "Epoch: 12/25... Step: 3340... Loss: 0.9419... Val Loss: 1.6821\n",
      "Epoch: 12/25... Step: 3350... Loss: 0.9809... Val Loss: 1.7196\n",
      "Epoch: 12/25... Step: 3360... Loss: 0.7632... Val Loss: 1.7308\n",
      "Epoch: 12/25... Step: 3370... Loss: 0.7891... Val Loss: 1.7631\n",
      "Epoch: 12/25... Step: 3380... Loss: 0.7736... Val Loss: 1.7254\n",
      "Epoch: 12/25... Step: 3390... Loss: 0.8647... Val Loss: 1.6940\n",
      "Epoch: 12/25... Step: 3400... Loss: 0.9414... Val Loss: 1.7469\n",
      "Epoch: 12/25... Step: 3410... Loss: 0.9776... Val Loss: 1.7192\n",
      "Epoch: 12/25... Step: 3420... Loss: 0.8716... Val Loss: 1.6809\n",
      "Epoch: 12/25... Step: 3430... Loss: 0.9233... Val Loss: 1.7351\n",
      "Epoch: 12/25... Step: 3440... Loss: 0.7372... Val Loss: 1.7530\n",
      "Epoch: 12/25... Step: 3450... Loss: 0.7391... Val Loss: 1.7361\n",
      "Epoch: 12/25... Step: 3460... Loss: 0.7898... Val Loss: 1.7465\n",
      "Epoch: 12/25... Step: 3470... Loss: 0.7715... Val Loss: 1.7103\n",
      "Epoch: 12/25... Step: 3480... Loss: 0.9859... Val Loss: 1.7047\n",
      "Epoch: 12/25... Step: 3490... Loss: 0.7885... Val Loss: 1.7014\n",
      "Epoch: 12/25... Step: 3500... Loss: 0.8535... Val Loss: 1.6984\n",
      "Epoch: 12/25... Step: 3510... Loss: 0.8890... Val Loss: 1.6576\n",
      "Epoch: 12/25... Step: 3520... Loss: 0.7656... Val Loss: 1.6972\n",
      "Epoch: 12/25... Step: 3530... Loss: 1.0084... Val Loss: 1.7069\n",
      "Epoch: 12/25... Step: 3540... Loss: 0.8769... Val Loss: 1.7421\n",
      "Epoch: 12/25... Step: 3550... Loss: 0.9122... Val Loss: 1.7262\n",
      "Epoch: 12/25... Step: 3560... Loss: 0.8527... Val Loss: 1.6908\n",
      "Epoch: 12/25... Step: 3570... Loss: 0.8645... Val Loss: 1.7420\n",
      "Epoch: 13/25... Step: 3580... Loss: 0.7415... Val Loss: 1.7295\n",
      "Epoch: 13/25... Step: 3590... Loss: 0.7101... Val Loss: 1.7512\n",
      "Epoch: 13/25... Step: 3600... Loss: 0.8187... Val Loss: 1.7297\n",
      "Epoch: 13/25... Step: 3610... Loss: 0.8597... Val Loss: 1.7449\n",
      "Epoch: 13/25... Step: 3620... Loss: 0.7305... Val Loss: 1.7422\n",
      "Epoch: 13/25... Step: 3630... Loss: 0.8646... Val Loss: 1.7874\n",
      "Epoch: 13/25... Step: 3640... Loss: 0.8162... Val Loss: 1.7024\n",
      "Epoch: 13/25... Step: 3650... Loss: 1.0803... Val Loss: 1.7423\n",
      "Epoch: 13/25... Step: 3660... Loss: 0.8067... Val Loss: 1.7440\n",
      "Epoch: 13/25... Step: 3670... Loss: 0.7726... Val Loss: 1.7714\n",
      "Epoch: 13/25... Step: 3680... Loss: 0.8313... Val Loss: 1.7291\n",
      "Epoch: 13/25... Step: 3690... Loss: 0.8421... Val Loss: 1.7875\n",
      "Epoch: 13/25... Step: 3700... Loss: 0.8478... Val Loss: 1.8014\n",
      "Epoch: 13/25... Step: 3710... Loss: 0.8511... Val Loss: 1.7596\n",
      "Epoch: 13/25... Step: 3720... Loss: 0.7882... Val Loss: 1.7165\n",
      "Epoch: 13/25... Step: 3730... Loss: 0.9654... Val Loss: 1.7881\n",
      "Epoch: 13/25... Step: 3740... Loss: 0.7330... Val Loss: 1.7954\n",
      "Epoch: 13/25... Step: 3750... Loss: 0.6199... Val Loss: 1.7896\n",
      "Epoch: 13/25... Step: 3760... Loss: 0.8542... Val Loss: 1.7840\n",
      "Epoch: 13/25... Step: 3770... Loss: 0.9003... Val Loss: 1.7341\n",
      "Epoch: 13/25... Step: 3780... Loss: 0.8155... Val Loss: 1.7227\n",
      "Epoch: 13/25... Step: 3790... Loss: 0.8421... Val Loss: 1.7084\n",
      "Epoch: 13/25... Step: 3800... Loss: 0.7882... Val Loss: 1.7080\n",
      "Epoch: 13/25... Step: 3810... Loss: 0.9963... Val Loss: 1.6719\n",
      "Epoch: 13/25... Step: 3820... Loss: 0.7591... Val Loss: 1.7263\n",
      "Epoch: 13/25... Step: 3830... Loss: 0.7461... Val Loss: 1.7383\n",
      "Epoch: 13/25... Step: 3840... Loss: 0.7875... Val Loss: 1.7615\n",
      "Epoch: 13/25... Step: 3850... Loss: 0.7383... Val Loss: 1.7390\n",
      "Epoch: 13/25... Step: 3860... Loss: 0.8456... Val Loss: 1.7362\n",
      "Epoch: 13/25... Step: 3870... Loss: 0.8520... Val Loss: 1.7467\n",
      "Epoch: 14/25... Step: 3880... Loss: 0.8273... Val Loss: 1.7669\n",
      "Epoch: 14/25... Step: 3890... Loss: 1.0843... Val Loss: 1.7238\n",
      "Epoch: 14/25... Step: 3900... Loss: 0.9166... Val Loss: 1.7397\n",
      "Epoch: 14/25... Step: 3910... Loss: 0.8704... Val Loss: 1.7148\n",
      "Epoch: 14/25... Step: 3920... Loss: 0.7041... Val Loss: 1.7641\n",
      "Epoch: 14/25... Step: 3930... Loss: 0.8386... Val Loss: 1.7654\n",
      "Epoch: 14/25... Step: 3940... Loss: 0.9388... Val Loss: 1.7390\n",
      "Epoch: 14/25... Step: 3950... Loss: 0.8505... Val Loss: 1.7530\n",
      "Epoch: 14/25... Step: 3960... Loss: 0.8037... Val Loss: 1.7875\n",
      "Epoch: 14/25... Step: 3970... Loss: 0.8246... Val Loss: 1.7646\n",
      "Epoch: 14/25... Step: 3980... Loss: 1.0155... Val Loss: 1.7260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/25... Step: 3990... Loss: 0.8003... Val Loss: 1.7717\n",
      "Epoch: 14/25... Step: 4000... Loss: 0.8845... Val Loss: 1.7736\n",
      "Epoch: 14/25... Step: 4010... Loss: 0.6979... Val Loss: 1.7523\n",
      "Epoch: 14/25... Step: 4020... Loss: 0.7941... Val Loss: 1.7261\n",
      "Epoch: 14/25... Step: 4030... Loss: 0.7081... Val Loss: 1.7678\n",
      "Epoch: 14/25... Step: 4040... Loss: 0.7126... Val Loss: 1.8138\n",
      "Epoch: 14/25... Step: 4050... Loss: 0.7774... Val Loss: 1.7988\n",
      "Epoch: 14/25... Step: 4060... Loss: 0.9230... Val Loss: 1.7896\n",
      "Epoch: 14/25... Step: 4070... Loss: 0.7760... Val Loss: 1.7591\n",
      "Epoch: 14/25... Step: 4080... Loss: 0.8882... Val Loss: 1.6998\n",
      "Epoch: 14/25... Step: 4090... Loss: 0.8183... Val Loss: 1.6846\n",
      "Epoch: 14/25... Step: 4100... Loss: 0.8198... Val Loss: 1.7089\n",
      "Epoch: 14/25... Step: 4110... Loss: 0.8521... Val Loss: 1.6779\n",
      "Epoch: 14/25... Step: 4120... Loss: 0.8922... Val Loss: 1.7482\n",
      "Epoch: 14/25... Step: 4130... Loss: 0.7692... Val Loss: 1.7283\n",
      "Epoch: 14/25... Step: 4140... Loss: 0.7466... Val Loss: 1.7363\n",
      "Epoch: 14/25... Step: 4150... Loss: 0.9128... Val Loss: 1.7404\n",
      "Epoch: 14/25... Step: 4160... Loss: 1.0664... Val Loss: 1.6950\n",
      "Epoch: 14/25... Step: 4170... Loss: 0.9486... Val Loss: 1.7305\n",
      "Epoch: 15/25... Step: 4180... Loss: 0.7406... Val Loss: 1.7909\n",
      "Epoch: 15/25... Step: 4190... Loss: 0.8377... Val Loss: 1.7129\n",
      "Epoch: 15/25... Step: 4200... Loss: 0.8250... Val Loss: 1.7192\n",
      "Epoch: 15/25... Step: 4210... Loss: 0.6274... Val Loss: 1.7192\n",
      "Epoch: 15/25... Step: 4220... Loss: 0.7866... Val Loss: 1.7751\n",
      "Epoch: 15/25... Step: 4230... Loss: 0.7551... Val Loss: 1.7421\n",
      "Epoch: 15/25... Step: 4240... Loss: 0.7532... Val Loss: 1.7236\n",
      "Epoch: 15/25... Step: 4250... Loss: 0.8526... Val Loss: 1.7620\n",
      "Epoch: 15/25... Step: 4260... Loss: 0.7135... Val Loss: 1.7845\n",
      "Epoch: 15/25... Step: 4270... Loss: 0.7488... Val Loss: 1.7960\n",
      "Epoch: 15/25... Step: 4280... Loss: 0.9409... Val Loss: 1.7777\n",
      "Epoch: 15/25... Step: 4290... Loss: 0.7884... Val Loss: 1.7785\n",
      "Epoch: 15/25... Step: 4300... Loss: 0.8223... Val Loss: 1.8116\n",
      "Epoch: 15/25... Step: 4310... Loss: 0.8112... Val Loss: 1.7593\n",
      "Epoch: 15/25... Step: 4320... Loss: 0.8105... Val Loss: 1.7464\n",
      "Epoch: 15/25... Step: 4330... Loss: 0.7091... Val Loss: 1.8076\n",
      "Epoch: 15/25... Step: 4340... Loss: 0.7483... Val Loss: 1.7813\n",
      "Epoch: 15/25... Step: 4350... Loss: 0.7256... Val Loss: 1.8103\n",
      "Epoch: 15/25... Step: 4360... Loss: 0.6996... Val Loss: 1.7747\n",
      "Epoch: 15/25... Step: 4370... Loss: 0.9297... Val Loss: 1.7585\n",
      "Epoch: 15/25... Step: 4380... Loss: 0.8690... Val Loss: 1.7285\n",
      "Epoch: 15/25... Step: 4390... Loss: 0.7575... Val Loss: 1.7001\n",
      "Epoch: 15/25... Step: 4400... Loss: 0.8178... Val Loss: 1.7288\n",
      "Epoch: 15/25... Step: 4410... Loss: 0.7860... Val Loss: 1.7057\n",
      "Epoch: 15/25... Step: 4420... Loss: 0.7666... Val Loss: 1.7478\n",
      "Epoch: 15/25... Step: 4430... Loss: 0.8418... Val Loss: 1.7353\n",
      "Epoch: 15/25... Step: 4440... Loss: 0.6800... Val Loss: 1.7615\n",
      "Epoch: 15/25... Step: 4450... Loss: 0.8762... Val Loss: 1.7495\n",
      "Epoch: 15/25... Step: 4460... Loss: 0.8055... Val Loss: 1.7211\n",
      "Epoch: 15/25... Step: 4470... Loss: 0.8771... Val Loss: 1.7328\n",
      "Epoch: 16/25... Step: 4480... Loss: 0.7071... Val Loss: 1.7718\n",
      "Epoch: 16/25... Step: 4490... Loss: 0.6557... Val Loss: 1.7426\n",
      "Epoch: 16/25... Step: 4500... Loss: 0.6513... Val Loss: 1.7420\n",
      "Epoch: 16/25... Step: 4510... Loss: 0.6984... Val Loss: 1.7717\n",
      "Epoch: 16/25... Step: 4520... Loss: 0.8307... Val Loss: 1.8561\n",
      "Epoch: 16/25... Step: 4530... Loss: 0.7810... Val Loss: 1.7543\n",
      "Epoch: 16/25... Step: 4540... Loss: 0.6705... Val Loss: 1.7573\n",
      "Epoch: 16/25... Step: 4550... Loss: 0.7022... Val Loss: 1.7827\n",
      "Epoch: 16/25... Step: 4560... Loss: 0.7453... Val Loss: 1.8083\n",
      "Epoch: 16/25... Step: 4570... Loss: 0.8471... Val Loss: 1.8154\n",
      "Epoch: 16/25... Step: 4580... Loss: 0.7935... Val Loss: 1.7469\n",
      "Epoch: 16/25... Step: 4590... Loss: 0.7533... Val Loss: 1.8143\n",
      "Epoch: 16/25... Step: 4600... Loss: 0.6707... Val Loss: 1.8247\n",
      "Epoch: 16/25... Step: 4610... Loss: 0.7473... Val Loss: 1.7627\n",
      "Epoch: 16/25... Step: 4620... Loss: 0.6251... Val Loss: 1.7438\n",
      "Epoch: 16/25... Step: 4630... Loss: 0.6564... Val Loss: 1.8102\n",
      "Epoch: 16/25... Step: 4640... Loss: 0.6978... Val Loss: 1.8396\n",
      "Epoch: 16/25... Step: 4650... Loss: 0.8659... Val Loss: 1.8438\n",
      "Epoch: 16/25... Step: 4660... Loss: 0.6693... Val Loss: 1.8353\n",
      "Epoch: 16/25... Step: 4670... Loss: 0.8121... Val Loss: 1.7980\n",
      "Epoch: 16/25... Step: 4680... Loss: 0.9218... Val Loss: 1.7503\n",
      "Epoch: 16/25... Step: 4690... Loss: 0.8604... Val Loss: 1.7353\n",
      "Epoch: 16/25... Step: 4700... Loss: 0.8134... Val Loss: 1.7606\n",
      "Epoch: 16/25... Step: 4710... Loss: 0.6649... Val Loss: 1.7183\n",
      "Epoch: 16/25... Step: 4720... Loss: 0.6142... Val Loss: 1.7977\n",
      "Epoch: 16/25... Step: 4730... Loss: 0.8209... Val Loss: 1.8034\n",
      "Epoch: 16/25... Step: 4740... Loss: 0.7461... Val Loss: 1.8243\n",
      "Epoch: 16/25... Step: 4750... Loss: 0.6734... Val Loss: 1.7798\n",
      "Epoch: 16/25... Step: 4760... Loss: 0.7683... Val Loss: 1.7315\n",
      "Epoch: 17/25... Step: 4770... Loss: 0.6880... Val Loss: 1.7867\n",
      "Epoch: 17/25... Step: 4780... Loss: 0.6684... Val Loss: 1.8330\n",
      "Epoch: 17/25... Step: 4790... Loss: 0.7390... Val Loss: 1.7749\n",
      "Epoch: 17/25... Step: 4800... Loss: 0.7669... Val Loss: 1.7821\n",
      "Epoch: 17/25... Step: 4810... Loss: 0.6513... Val Loss: 1.7929\n",
      "Epoch: 17/25... Step: 4820... Loss: 0.7643... Val Loss: 1.8209\n",
      "Epoch: 17/25... Step: 4830... Loss: 0.8609... Val Loss: 1.7704\n",
      "Epoch: 17/25... Step: 4840... Loss: 0.8533... Val Loss: 1.7571\n",
      "Epoch: 17/25... Step: 4850... Loss: 0.6363... Val Loss: 1.7902\n",
      "Epoch: 17/25... Step: 4860... Loss: 0.6712... Val Loss: 1.8484\n",
      "Epoch: 17/25... Step: 4870... Loss: 0.7006... Val Loss: 1.8086\n",
      "Epoch: 17/25... Step: 4880... Loss: 0.6888... Val Loss: 1.7806\n",
      "Epoch: 17/25... Step: 4890... Loss: 0.7618... Val Loss: 1.8284\n",
      "Epoch: 17/25... Step: 4900... Loss: 0.8613... Val Loss: 1.8467\n",
      "Epoch: 17/25... Step: 4910... Loss: 0.7077... Val Loss: 1.8359\n",
      "Epoch: 17/25... Step: 4920... Loss: 0.8551... Val Loss: 1.7867\n",
      "Epoch: 17/25... Step: 4930... Loss: 0.6652... Val Loss: 1.8392\n",
      "Epoch: 17/25... Step: 4940... Loss: 0.6600... Val Loss: 1.8708\n",
      "Epoch: 17/25... Step: 4950... Loss: 0.6705... Val Loss: 1.8547\n",
      "Epoch: 17/25... Step: 4960... Loss: 0.6651... Val Loss: 1.8639\n",
      "Epoch: 17/25... Step: 4970... Loss: 0.8371... Val Loss: 1.8376\n",
      "Epoch: 17/25... Step: 4980... Loss: 0.6787... Val Loss: 1.7802\n",
      "Epoch: 17/25... Step: 4990... Loss: 0.7465... Val Loss: 1.7341\n",
      "Epoch: 17/25... Step: 5000... Loss: 0.7612... Val Loss: 1.7493\n",
      "Epoch: 17/25... Step: 5010... Loss: 0.6778... Val Loss: 1.7589\n",
      "Epoch: 17/25... Step: 5020... Loss: 0.8687... Val Loss: 1.8061\n",
      "Epoch: 17/25... Step: 5030... Loss: 0.7419... Val Loss: 1.7901\n",
      "Epoch: 17/25... Step: 5040... Loss: 0.7528... Val Loss: 1.8123\n",
      "Epoch: 17/25... Step: 5050... Loss: 0.7612... Val Loss: 1.7849\n",
      "Epoch: 17/25... Step: 5060... Loss: 0.7462... Val Loss: 1.7755\n",
      "Epoch: 18/25... Step: 5070... Loss: 0.6400... Val Loss: 1.8105\n",
      "Epoch: 18/25... Step: 5080... Loss: 0.5735... Val Loss: 1.8347\n",
      "Epoch: 18/25... Step: 5090... Loss: 0.7644... Val Loss: 1.8074\n",
      "Epoch: 18/25... Step: 5100... Loss: 0.7164... Val Loss: 1.7898\n",
      "Epoch: 18/25... Step: 5110... Loss: 0.6061... Val Loss: 1.8300\n",
      "Epoch: 18/25... Step: 5120... Loss: 0.7361... Val Loss: 1.8831\n",
      "Epoch: 18/25... Step: 5130... Loss: 0.6871... Val Loss: 1.7856\n",
      "Epoch: 18/25... Step: 5140... Loss: 0.9468... Val Loss: 1.7803\n",
      "Epoch: 18/25... Step: 5150... Loss: 0.7467... Val Loss: 1.8104\n",
      "Epoch: 18/25... Step: 5160... Loss: 0.6911... Val Loss: 1.8655\n",
      "Epoch: 18/25... Step: 5170... Loss: 0.6852... Val Loss: 1.8287\n",
      "Epoch: 18/25... Step: 5180... Loss: 0.7431... Val Loss: 1.8278\n",
      "Epoch: 18/25... Step: 5190... Loss: 0.8094... Val Loss: 1.8701\n",
      "Epoch: 18/25... Step: 5200... Loss: 0.7755... Val Loss: 1.8483\n",
      "Epoch: 18/25... Step: 5210... Loss: 0.7147... Val Loss: 1.8244\n",
      "Epoch: 18/25... Step: 5220... Loss: 0.7655... Val Loss: 1.8177\n",
      "Epoch: 18/25... Step: 5230... Loss: 0.5898... Val Loss: 1.8815\n",
      "Epoch: 18/25... Step: 5240... Loss: 0.5304... Val Loss: 1.8825\n",
      "Epoch: 18/25... Step: 5250... Loss: 0.8087... Val Loss: 1.8881\n",
      "Epoch: 18/25... Step: 5260... Loss: 0.7514... Val Loss: 1.7938\n",
      "Epoch: 18/25... Step: 5270... Loss: 0.7423... Val Loss: 1.7930\n",
      "Epoch: 18/25... Step: 5280... Loss: 0.7746... Val Loss: 1.7547\n",
      "Epoch: 18/25... Step: 5290... Loss: 0.7123... Val Loss: 1.7284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/25... Step: 5300... Loss: 0.8838... Val Loss: 1.7483\n",
      "Epoch: 18/25... Step: 5310... Loss: 0.6798... Val Loss: 1.7853\n",
      "Epoch: 18/25... Step: 5320... Loss: 0.6859... Val Loss: 1.8355\n",
      "Epoch: 18/25... Step: 5330... Loss: 0.6858... Val Loss: 1.8288\n",
      "Epoch: 18/25... Step: 5340... Loss: 0.6159... Val Loss: 1.8124\n",
      "Epoch: 18/25... Step: 5350... Loss: 0.7209... Val Loss: 1.8015\n",
      "Epoch: 18/25... Step: 5360... Loss: 0.7261... Val Loss: 1.7811\n",
      "Epoch: 19/25... Step: 5370... Loss: 0.6985... Val Loss: 1.8407\n",
      "Epoch: 19/25... Step: 5380... Loss: 0.8782... Val Loss: 1.8971\n",
      "Epoch: 19/25... Step: 5390... Loss: 0.7805... Val Loss: 1.8077\n",
      "Epoch: 19/25... Step: 5400... Loss: 0.6960... Val Loss: 1.8213\n",
      "Epoch: 19/25... Step: 5410... Loss: 0.6038... Val Loss: 1.8428\n",
      "Epoch: 19/25... Step: 5420... Loss: 0.6882... Val Loss: 1.8982\n",
      "Epoch: 19/25... Step: 5430... Loss: 0.7982... Val Loss: 1.8297\n",
      "Epoch: 19/25... Step: 5440... Loss: 0.7573... Val Loss: 1.7898\n",
      "Epoch: 19/25... Step: 5450... Loss: 0.7558... Val Loss: 1.8288\n",
      "Epoch: 19/25... Step: 5460... Loss: 0.7118... Val Loss: 1.9280\n",
      "Epoch: 19/25... Step: 5470... Loss: 0.8945... Val Loss: 1.8301\n",
      "Epoch: 19/25... Step: 5480... Loss: 0.7206... Val Loss: 1.8178\n",
      "Epoch: 19/25... Step: 5490... Loss: 0.7804... Val Loss: 1.8942\n",
      "Epoch: 19/25... Step: 5500... Loss: 0.6091... Val Loss: 1.8665\n",
      "Epoch: 19/25... Step: 5510... Loss: 0.6788... Val Loss: 1.8179\n",
      "Epoch: 19/25... Step: 5520... Loss: 0.6385... Val Loss: 1.8227\n",
      "Epoch: 19/25... Step: 5530... Loss: 0.6544... Val Loss: 1.9218\n",
      "Epoch: 19/25... Step: 5540... Loss: 0.6760... Val Loss: 1.9321\n",
      "Epoch: 19/25... Step: 5550... Loss: 0.7835... Val Loss: 1.9185\n",
      "Epoch: 19/25... Step: 5560... Loss: 0.7119... Val Loss: 1.8596\n",
      "Epoch: 19/25... Step: 5570... Loss: 0.8010... Val Loss: 1.8432\n",
      "Epoch: 19/25... Step: 5580... Loss: 0.7385... Val Loss: 1.8090\n",
      "Epoch: 19/25... Step: 5590... Loss: 0.7244... Val Loss: 1.7592\n",
      "Epoch: 19/25... Step: 5600... Loss: 0.7642... Val Loss: 1.7619\n",
      "Epoch: 19/25... Step: 5610... Loss: 0.7957... Val Loss: 1.8018\n",
      "Epoch: 19/25... Step: 5620... Loss: 0.6668... Val Loss: 1.8111\n",
      "Epoch: 19/25... Step: 5630... Loss: 0.6969... Val Loss: 1.8645\n",
      "Epoch: 19/25... Step: 5640... Loss: 0.8302... Val Loss: 1.8342\n",
      "Epoch: 19/25... Step: 5650... Loss: 0.9064... Val Loss: 1.8438\n",
      "Epoch: 19/25... Step: 5660... Loss: 0.8629... Val Loss: 1.8299\n",
      "Epoch: 20/25... Step: 5670... Loss: 0.7042... Val Loss: 1.8531\n",
      "Epoch: 20/25... Step: 5680... Loss: 0.7915... Val Loss: 1.8693\n",
      "Epoch: 20/25... Step: 5690... Loss: 0.7335... Val Loss: 1.8523\n",
      "Epoch: 20/25... Step: 5700... Loss: 0.5859... Val Loss: 1.8495\n",
      "Epoch: 20/25... Step: 5710... Loss: 0.6416... Val Loss: 1.8716\n",
      "Epoch: 20/25... Step: 5720... Loss: 0.6854... Val Loss: 1.8937\n",
      "Epoch: 20/25... Step: 5730... Loss: 0.6847... Val Loss: 1.8230\n",
      "Epoch: 20/25... Step: 5740... Loss: 0.7209... Val Loss: 1.8095\n",
      "Epoch: 20/25... Step: 5750... Loss: 0.6753... Val Loss: 1.8492\n",
      "Epoch: 20/25... Step: 5760... Loss: 0.6356... Val Loss: 1.8839\n",
      "Epoch: 20/25... Step: 5770... Loss: 0.7285... Val Loss: 1.8596\n",
      "Epoch: 20/25... Step: 5780... Loss: 0.7158... Val Loss: 1.8775\n",
      "Epoch: 20/25... Step: 5790... Loss: 0.7523... Val Loss: 1.9050\n",
      "Epoch: 20/25... Step: 5800... Loss: 0.7075... Val Loss: 1.8818\n",
      "Epoch: 20/25... Step: 5810... Loss: 0.7533... Val Loss: 1.8519\n",
      "Epoch: 20/25... Step: 5820... Loss: 0.6211... Val Loss: 1.8303\n",
      "Epoch: 20/25... Step: 5830... Loss: 0.6420... Val Loss: 1.9066\n",
      "Epoch: 20/25... Step: 5840... Loss: 0.6158... Val Loss: 1.9436\n",
      "Epoch: 20/25... Step: 5850... Loss: 0.5717... Val Loss: 1.9323\n",
      "Epoch: 20/25... Step: 5860... Loss: 0.8568... Val Loss: 1.8696\n",
      "Epoch: 20/25... Step: 5870... Loss: 0.7007... Val Loss: 1.8591\n",
      "Epoch: 20/25... Step: 5880... Loss: 0.6910... Val Loss: 1.8121\n",
      "Epoch: 20/25... Step: 5890... Loss: 0.7386... Val Loss: 1.7929\n",
      "Epoch: 20/25... Step: 5900... Loss: 0.7224... Val Loss: 1.7910\n",
      "Epoch: 20/25... Step: 5910... Loss: 0.7030... Val Loss: 1.8687\n",
      "Epoch: 20/25... Step: 5920... Loss: 0.7218... Val Loss: 1.8651\n",
      "Epoch: 20/25... Step: 5930... Loss: 0.5903... Val Loss: 1.8801\n",
      "Epoch: 20/25... Step: 5940... Loss: 0.8076... Val Loss: 1.8829\n",
      "Epoch: 20/25... Step: 5950... Loss: 0.7022... Val Loss: 1.8813\n",
      "Epoch: 20/25... Step: 5960... Loss: 0.7687... Val Loss: 1.8314\n",
      "Epoch: 21/25... Step: 5970... Loss: 0.6191... Val Loss: 1.8823\n",
      "Epoch: 21/25... Step: 5980... Loss: 0.5854... Val Loss: 1.8867\n",
      "Epoch: 21/25... Step: 5990... Loss: 0.6078... Val Loss: 1.8735\n",
      "Epoch: 21/25... Step: 6000... Loss: 0.6138... Val Loss: 1.8848\n",
      "Epoch: 21/25... Step: 6010... Loss: 0.6710... Val Loss: 1.8683\n",
      "Epoch: 21/25... Step: 6020... Loss: 0.6683... Val Loss: 1.9189\n",
      "Epoch: 21/25... Step: 6030... Loss: 0.5703... Val Loss: 1.8590\n",
      "Epoch: 21/25... Step: 6040... Loss: 0.6306... Val Loss: 1.8640\n",
      "Epoch: 21/25... Step: 6050... Loss: 0.6668... Val Loss: 1.8827\n",
      "Epoch: 21/25... Step: 6060... Loss: 0.7491... Val Loss: 1.9326\n",
      "Epoch: 21/25... Step: 6070... Loss: 0.6642... Val Loss: 1.8480\n",
      "Epoch: 21/25... Step: 6080... Loss: 0.6041... Val Loss: 1.8941\n",
      "Epoch: 21/25... Step: 6090... Loss: 0.5943... Val Loss: 1.9752\n",
      "Epoch: 21/25... Step: 6100... Loss: 0.6680... Val Loss: 1.8936\n",
      "Epoch: 21/25... Step: 6110... Loss: 0.5441... Val Loss: 1.8586\n",
      "Epoch: 21/25... Step: 6120... Loss: 0.5634... Val Loss: 1.8733\n",
      "Epoch: 21/25... Step: 6130... Loss: 0.6393... Val Loss: 1.9415\n",
      "Epoch: 21/25... Step: 6140... Loss: 0.7252... Val Loss: 1.9579\n",
      "Epoch: 21/25... Step: 6150... Loss: 0.5952... Val Loss: 1.9165\n",
      "Epoch: 21/25... Step: 6160... Loss: 0.6684... Val Loss: 1.9060\n",
      "Epoch: 21/25... Step: 6170... Loss: 0.7660... Val Loss: 1.8621\n",
      "Epoch: 21/25... Step: 6180... Loss: 0.8239... Val Loss: 1.8558\n",
      "Epoch: 21/25... Step: 6190... Loss: 0.7507... Val Loss: 1.8422\n",
      "Epoch: 21/25... Step: 6200... Loss: 0.6349... Val Loss: 1.7767\n",
      "Epoch: 21/25... Step: 6210... Loss: 0.5691... Val Loss: 1.8491\n",
      "Epoch: 21/25... Step: 6220... Loss: 0.7655... Val Loss: 1.8731\n",
      "Epoch: 21/25... Step: 6230... Loss: 0.6549... Val Loss: 1.9388\n",
      "Epoch: 21/25... Step: 6240... Loss: 0.5889... Val Loss: 1.8827\n",
      "Epoch: 21/25... Step: 6250... Loss: 0.7324... Val Loss: 1.8722\n",
      "Epoch: 22/25... Step: 6260... Loss: 0.6410... Val Loss: 1.8915\n",
      "Epoch: 22/25... Step: 6270... Loss: 0.6059... Val Loss: 1.9689\n",
      "Epoch: 22/25... Step: 6280... Loss: 0.6623... Val Loss: 1.9650\n",
      "Epoch: 22/25... Step: 6290... Loss: 0.6634... Val Loss: 1.9091\n",
      "Epoch: 22/25... Step: 6300... Loss: 0.5950... Val Loss: 1.9230\n",
      "Epoch: 22/25... Step: 6310... Loss: 0.6200... Val Loss: 1.9817\n",
      "Epoch: 22/25... Step: 6320... Loss: 0.7203... Val Loss: 1.9381\n",
      "Epoch: 22/25... Step: 6330... Loss: 0.7760... Val Loss: 1.9295\n",
      "Epoch: 22/25... Step: 6340... Loss: 0.5575... Val Loss: 1.8759\n",
      "Epoch: 22/25... Step: 6350... Loss: 0.5934... Val Loss: 1.9110\n",
      "Epoch: 22/25... Step: 6360... Loss: 0.5865... Val Loss: 1.9619\n",
      "Epoch: 22/25... Step: 6370... Loss: 0.6458... Val Loss: 1.9159\n",
      "Epoch: 22/25... Step: 6380... Loss: 0.7381... Val Loss: 1.9127\n",
      "Epoch: 22/25... Step: 6390... Loss: 0.7433... Val Loss: 1.9106\n",
      "Epoch: 22/25... Step: 6400... Loss: 0.6828... Val Loss: 1.8947\n",
      "Epoch: 22/25... Step: 6410... Loss: 0.7561... Val Loss: 1.8827\n",
      "Epoch: 22/25... Step: 6420... Loss: 0.5464... Val Loss: 1.8853\n",
      "Epoch: 22/25... Step: 6430... Loss: 0.5425... Val Loss: 1.9550\n",
      "Epoch: 22/25... Step: 6440... Loss: 0.6312... Val Loss: 1.9986\n",
      "Epoch: 22/25... Step: 6450... Loss: 0.6049... Val Loss: 1.9236\n",
      "Epoch: 22/25... Step: 6460... Loss: 0.7984... Val Loss: 1.9033\n",
      "Epoch: 22/25... Step: 6470... Loss: 0.6366... Val Loss: 1.8596\n",
      "Epoch: 22/25... Step: 6480... Loss: 0.7281... Val Loss: 1.8158\n",
      "Epoch: 22/25... Step: 6490... Loss: 0.6653... Val Loss: 1.8330\n",
      "Epoch: 22/25... Step: 6500... Loss: 0.5914... Val Loss: 1.8220\n",
      "Epoch: 22/25... Step: 6510... Loss: 0.7331... Val Loss: 1.8465\n",
      "Epoch: 22/25... Step: 6520... Loss: 0.6597... Val Loss: 1.9148\n",
      "Epoch: 22/25... Step: 6530... Loss: 0.7010... Val Loss: 1.9790\n",
      "Epoch: 22/25... Step: 6540... Loss: 0.6819... Val Loss: 1.9448\n",
      "Epoch: 22/25... Step: 6550... Loss: 0.6414... Val Loss: 1.8989\n",
      "Epoch: 23/25... Step: 6560... Loss: 0.5776... Val Loss: 1.9178\n",
      "Epoch: 23/25... Step: 6570... Loss: 0.5424... Val Loss: 1.9987\n",
      "Epoch: 23/25... Step: 6580... Loss: 0.6312... Val Loss: 1.9984\n",
      "Epoch: 23/25... Step: 6590... Loss: 0.6471... Val Loss: 1.9447\n",
      "Epoch: 23/25... Step: 6600... Loss: 0.5478... Val Loss: 1.9267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/25... Step: 6610... Loss: 0.6313... Val Loss: 1.9865\n",
      "Epoch: 23/25... Step: 6620... Loss: 0.6260... Val Loss: 1.9282\n",
      "Epoch: 23/25... Step: 6630... Loss: 0.8103... Val Loss: 1.8960\n",
      "Epoch: 23/25... Step: 6640... Loss: 0.6203... Val Loss: 1.9500\n",
      "Epoch: 23/25... Step: 6650... Loss: 0.5773... Val Loss: 1.9441\n",
      "Epoch: 23/25... Step: 6660... Loss: 0.6413... Val Loss: 1.9466\n",
      "Epoch: 23/25... Step: 6670... Loss: 0.7009... Val Loss: 1.9344\n",
      "Epoch: 23/25... Step: 6680... Loss: 0.7015... Val Loss: 1.9744\n",
      "Epoch: 23/25... Step: 6690... Loss: 0.6468... Val Loss: 1.9688\n",
      "Epoch: 23/25... Step: 6700... Loss: 0.6273... Val Loss: 1.9437\n",
      "Epoch: 23/25... Step: 6710... Loss: 0.6839... Val Loss: 1.9111\n",
      "Epoch: 23/25... Step: 6720... Loss: 0.5294... Val Loss: 1.9248\n",
      "Epoch: 23/25... Step: 6730... Loss: 0.4539... Val Loss: 1.9954\n",
      "Epoch: 23/25... Step: 6740... Loss: 0.7205... Val Loss: 2.0085\n",
      "Epoch: 23/25... Step: 6750... Loss: 0.7168... Val Loss: 1.9608\n",
      "Epoch: 23/25... Step: 6760... Loss: 0.6201... Val Loss: 1.9409\n",
      "Epoch: 23/25... Step: 6770... Loss: 0.6758... Val Loss: 1.9060\n",
      "Epoch: 23/25... Step: 6780... Loss: 0.6311... Val Loss: 1.8378\n",
      "Epoch: 23/25... Step: 6790... Loss: 0.7643... Val Loss: 1.8634\n",
      "Epoch: 23/25... Step: 6800... Loss: 0.5928... Val Loss: 1.8797\n",
      "Epoch: 23/25... Step: 6810... Loss: 0.5758... Val Loss: 1.8947\n",
      "Epoch: 23/25... Step: 6820... Loss: 0.6583... Val Loss: 1.9274\n",
      "Epoch: 23/25... Step: 6830... Loss: 0.5622... Val Loss: 1.9402\n",
      "Epoch: 23/25... Step: 6840... Loss: 0.6779... Val Loss: 1.9458\n",
      "Epoch: 23/25... Step: 6850... Loss: 0.6506... Val Loss: 1.9211\n",
      "Epoch: 24/25... Step: 6860... Loss: 0.6106... Val Loss: 1.9793\n",
      "Epoch: 24/25... Step: 6870... Loss: 0.6893... Val Loss: 1.9958\n",
      "Epoch: 24/25... Step: 6880... Loss: 0.6310... Val Loss: 2.0043\n",
      "Epoch: 24/25... Step: 6890... Loss: 0.7023... Val Loss: 1.9688\n",
      "Epoch: 24/25... Step: 6900... Loss: 0.5833... Val Loss: 1.9537\n",
      "Epoch: 24/25... Step: 6910... Loss: 0.6203... Val Loss: 2.0166\n",
      "Epoch: 24/25... Step: 6920... Loss: 0.6615... Val Loss: 2.0101\n",
      "Epoch: 24/25... Step: 6930... Loss: 0.6863... Val Loss: 1.9856\n",
      "Epoch: 24/25... Step: 6940... Loss: 0.6465... Val Loss: 1.9521\n",
      "Epoch: 24/25... Step: 6950... Loss: 0.6243... Val Loss: 1.9981\n",
      "Epoch: 24/25... Step: 6960... Loss: 0.7537... Val Loss: 2.0132\n",
      "Epoch: 24/25... Step: 6970... Loss: 0.6258... Val Loss: 1.9743\n",
      "Epoch: 24/25... Step: 6980... Loss: 0.6380... Val Loss: 1.9899\n",
      "Epoch: 24/25... Step: 6990... Loss: 0.5498... Val Loss: 2.0187\n",
      "Epoch: 24/25... Step: 7000... Loss: 0.6272... Val Loss: 1.9982\n",
      "Epoch: 24/25... Step: 7010... Loss: 0.5419... Val Loss: 1.9582\n",
      "Epoch: 24/25... Step: 7020... Loss: 0.5585... Val Loss: 1.9554\n",
      "Epoch: 24/25... Step: 7030... Loss: 0.5967... Val Loss: 2.0320\n",
      "Epoch: 24/25... Step: 7040... Loss: 0.7155... Val Loss: 2.0240\n",
      "Epoch: 24/25... Step: 7050... Loss: 0.6479... Val Loss: 1.9685\n",
      "Epoch: 24/25... Step: 7060... Loss: 0.6690... Val Loss: 1.9493\n",
      "Epoch: 24/25... Step: 7070... Loss: 0.6959... Val Loss: 1.8814\n",
      "Epoch: 24/25... Step: 7080... Loss: 0.6386... Val Loss: 1.8820\n",
      "Epoch: 24/25... Step: 7090... Loss: 0.6624... Val Loss: 1.8787\n",
      "Epoch: 24/25... Step: 7100... Loss: 0.6896... Val Loss: 1.8969\n",
      "Epoch: 24/25... Step: 7110... Loss: 0.6171... Val Loss: 1.9482\n",
      "Epoch: 24/25... Step: 7120... Loss: 0.5896... Val Loss: 1.9630\n",
      "Epoch: 24/25... Step: 7130... Loss: 0.6906... Val Loss: 2.0044\n",
      "Epoch: 24/25... Step: 7140... Loss: 0.7497... Val Loss: 1.9809\n",
      "Epoch: 24/25... Step: 7150... Loss: 0.7164... Val Loss: 1.9293\n",
      "Epoch: 25/25... Step: 7160... Loss: 0.6098... Val Loss: 1.9768\n",
      "Epoch: 25/25... Step: 7170... Loss: 0.6015... Val Loss: 2.0743\n",
      "Epoch: 25/25... Step: 7180... Loss: 0.6237... Val Loss: 2.0686\n",
      "Epoch: 25/25... Step: 7190... Loss: 0.5286... Val Loss: 1.9950\n",
      "Epoch: 25/25... Step: 7200... Loss: 0.5797... Val Loss: 1.9886\n",
      "Epoch: 25/25... Step: 7210... Loss: 0.6146... Val Loss: 2.0426\n",
      "Epoch: 25/25... Step: 7220... Loss: 0.6201... Val Loss: 2.0235\n",
      "Epoch: 25/25... Step: 7230... Loss: 0.6373... Val Loss: 1.9435\n",
      "Epoch: 25/25... Step: 7240... Loss: 0.6059... Val Loss: 1.9501\n",
      "Epoch: 25/25... Step: 7250... Loss: 0.5686... Val Loss: 1.9694\n",
      "Epoch: 25/25... Step: 7260... Loss: 0.6925... Val Loss: 2.0020\n",
      "Epoch: 25/25... Step: 7270... Loss: 0.6323... Val Loss: 1.9728\n",
      "Epoch: 25/25... Step: 7280... Loss: 0.6317... Val Loss: 2.0050\n",
      "Epoch: 25/25... Step: 7290... Loss: 0.6505... Val Loss: 2.0272\n",
      "Epoch: 25/25... Step: 7300... Loss: 0.6561... Val Loss: 1.9883\n",
      "Epoch: 25/25... Step: 7310... Loss: 0.5596... Val Loss: 2.0094\n",
      "Epoch: 25/25... Step: 7320... Loss: 0.5595... Val Loss: 2.0137\n",
      "Epoch: 25/25... Step: 7330... Loss: 0.5910... Val Loss: 2.0498\n",
      "Epoch: 25/25... Step: 7340... Loss: 0.5682... Val Loss: 2.0325\n",
      "Epoch: 25/25... Step: 7350... Loss: 0.7509... Val Loss: 1.9712\n",
      "Epoch: 25/25... Step: 7360... Loss: 0.6374... Val Loss: 1.9586\n",
      "Epoch: 25/25... Step: 7370... Loss: 0.6169... Val Loss: 1.9583\n",
      "Epoch: 25/25... Step: 7380... Loss: 0.6341... Val Loss: 1.9571\n",
      "Epoch: 25/25... Step: 7390... Loss: 0.6319... Val Loss: 1.9411\n",
      "Epoch: 25/25... Step: 7400... Loss: 0.6175... Val Loss: 1.9796\n",
      "Epoch: 25/25... Step: 7410... Loss: 0.5923... Val Loss: 1.9830\n",
      "Epoch: 25/25... Step: 7420... Loss: 0.5213... Val Loss: 1.9622\n",
      "Epoch: 25/25... Step: 7430... Loss: 0.7255... Val Loss: 2.0021\n",
      "Epoch: 25/25... Step: 7440... Loss: 0.6396... Val Loss: 2.0056\n",
      "Epoch: 25/25... Step: 7450... Loss: 0.6366... Val Loss: 1.9957\n"
     ]
    }
   ],
   "source": [
    "if 'net' in locals():\n",
    "    del net\n",
    "\n",
    "\n",
    "\n",
    "net = CharRNN(chars, n_hidden=512, n_layers=2)\n",
    "\n",
    "n_seqs, n_steps = 10, 50\n",
    "train(net, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "with open('rnn.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltb3/.conda/envs/mwo-lstm/lib/python3.6/site-packages/ipykernel_launcher.py:49: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/ltb3/.conda/envs/mwo-lstm/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/ltb3/.conda/envs/mwo-lstm/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rear oil cooler shd24\n",
      "  changeout ~ right slew motor has failed\n",
      "  repair hyd leak on hy distribution blocks\n",
      "  repair cracks in control\n",
      "  broken bolts on cab mounts\n",
      "  replace rh eng coolant temp sensor\n",
      "  replace r/h slew motor hoses\n",
      "  replace r/h slew motor\n",
      "  hydraulic leak on rh engine\n",
      "  repair oil leak on r/h slew motor hoses\n",
      "  changeout rh no1 track roller\n",
      "  change out l/h slew motor\n",
      "  repair oil leak\n",
      "  replace hyd hose\n",
      "  replace rhs slew motor\n",
      "  repair oil leak on boom\n",
      "  changeout ~ right hand track chain\n",
      "  change out rh slew motor\n",
      "  changeout ~ left hand slew motor\n",
      "  change out r/h engine\n",
      "  rh eng oil pump spline cavities & fit new bolts\n",
      "  changeout rh slew motor\n",
      "  repairs\n",
      "  hyd leak\n",
      "  repair hydraulic leak\n",
      "  replace hyd hose on shutting dramas\n",
      "  replace broken bolts\n",
      "  replace rh slew motor hoses\n",
      "  clamps on hand rail broken.\n",
      "  cab mount bolt missing on bucket\n",
      "  replace bucket teeth\n",
      "  replace bucket tooth and keepers\n",
      "  replace bucket tooth and keepers\n",
      "  leaking hyd hose on starter\n",
      "  broken button in cab\n",
      "  repair cameras\n",
      "  harness & battery cable repairs\n",
      "  replace rh slew motor hose\n",
      "  replace rh slew motor\n",
      "  hydraulic oil leak\n",
      "  repair oil leak on l/h slew motor\n",
      "  repair hydraulic leaks see text\n",
      "  change out l/h engine overheating\n",
      "  replace blown hydraulic hose\n",
      "  repair hyd oil leak\n",
      "  replace r/h engine temp. gauge up\n",
      "  changeout ~ left hand track chain changeout\n",
      "  rh120c-lh slew motor changeout\n",
      "  rh120c-lh servo pump changeout\n",
      "  reseal rh slew motor hose\n",
      "  replace r/h slew motor hoses\n",
      "  repair oil leak at hydraulic oil coolers\n",
      "  changeout ~ left hand track adjuster\n",
      "  repair oil leak at hyd leak\n",
      "  repair hydraulic oil leak\n",
      "  hydraulic oil leak\n",
      "  replace hose on slew missor\n",
      "  broken bolts on cab bolt\n",
      "  replace blown hose\n",
      "  hydraulic oil leak\n",
      "  hydraulic leak\n",
      "  hyd leak at rh slew motor\n",
      "  hoses leaking at bolts broken.\n",
      "  cab mount bolts broken\n",
      "  repair crack in both engine bay door handle lh servo pump\n",
      "  replace broken bolts in cab mounts\n",
      "  replace r/h slew motor hoses broken\n",
      "  change out r/h slew motor hose\n",
      "  changeout rh no1 track roller - cw\n",
      "  c/out rh no-6 track roller- cw\n",
      "  change out rh no-7 track roller - cw\n",
      "  changeout rh no-7 track roller\n",
      "  changeout ~ left hand track chain\n",
      "  replace blown hyd hose\n",
      "  reseal #1 main pump\n",
      "  changeout rh no-7 track roller\n",
      "  changeout ~ right slew motor\n",
      "  changeout ~ left hand slew gearcase\n",
      "  changeout ~ right slew motor changeout\n",
      "  rh120c-lh slew motor changeout\n",
      "  rh120c-rh slew gear box chip detector on shd24\n",
      "  repair camera system\n",
      "  replace rh slew motor hose\n",
      "  changeout rh no1 track roller\n",
      "  check air filters on bottom cylinder\n",
      "  rh slew pump cont of stick\n",
      "  replace broken grease lines\n",
      "  replace l/h stick cylinder pin\n",
      "  repair hydraulic leak\n",
      "  hyd hose leaking\n",
      "  rh eng oil pressure low\n",
      "  lube fault - line b cable\n",
      "  repair cracks in comeng on\n",
      "  rh engine cooler plates\n",
      "  rh slew motor mounting broken.\n",
      "  repair hyd leak on boom completed\n",
      "  repair leak on r/h fuel tank sender faulty - rh pressure low\n",
      "  leaking on hydraulic fan pump\n",
      "  changeout ~ left hand track adjuster\n",
      "  changeout ~ right hand tooth\n",
      "  replace bucket teeth\n",
      "  replace tooth\n",
      "  lost tooth on bucket\n",
      "  broken grease line on bucket\n",
      "  repair bulks not working\n",
      "  replace r/h servo pump\n",
      "  changeout ~ left slew motor\n",
      "  change out rh no-6 track roller\n",
      "  changeout ~ right hand track adjuster hose\n",
      "  changeout ~ left hand slew gearcase\n",
      "  changeout ~ right hand track adjuster\n",
      "  rh slew motor hose blown\n",
      "  replace broken grease fitting\n",
      "  broken grease lines on bucket\n",
      "  replace broken grease line on bucket\n",
      "  repair cracks in bucket pin\n",
      "  lost tooth shd24\n",
      "  replace bucket teeth & keepers\n",
      "  creep undercarriage roller shd24\n",
      "  replace r/h slew motor\n",
      "  repair hyd leak\n",
      "  repair hydraulic oil leak\n",
      "  replace r/h slew motor\n",
      "  hydraulic oil leak\n",
      "  hydraulic oil leak @ replace\n",
      "  repair oil leak at front carry\n",
      "  change out r/h servo/pto lube pump\n",
      "  grease line on bucket pin\n",
      "  repair cracks in cab\n",
      "  repair crack in batteries and jump start shovel\n",
      "  jump start machine\n",
      "  jump start shovel\n",
      "  engine will not start\n",
      "  r/h engine shut down\n",
      "  engines won't start\n",
      "  jump start\n",
      "  jump start\n",
      "  jump start\n",
      "  jump start\n",
      "  jump start\n",
      "  jump start machine\n",
      "  jump start mechine\n",
      "  lube system failure\n",
      "  repair camera system\n",
      "  replace r/h engine oil filters\n",
      "  replace r/h fuel tank sender failed\n",
      "  changeout ~ right slew motor hose\n",
      "  changeout rh no-7 track roller\n",
      "  changeout ~ right hand slew gearcase\n",
      "  changeout ~ left hand track chain\n",
      "  changeout ~ right hand track adjuster hose shd24\n",
      "  changeout ~ lift propel brakes shd24\n",
      "  reseal rh engine fault shd24\n",
      "  lube fault - lost cover plate\n",
      "  replace blown hyd hose\n",
      "  oil leaks hp filters\n",
      "  replace rh engine cameras\n",
      "  rh engine cut out come off\n",
      "  changeout ~ left hand track chain\n",
      "  changeout ~ right slew motor\n",
      "  replace hose blown on cab\n",
      "  changeout rh near turbo elbow\n",
      "  change out l/h engine temp gauge not working\n",
      "  repair hydraulic leaks\n",
      "  repair hyd leak on hy faulty\n",
      "  replace blown hose\n",
      "  replace broken grease lines\n",
      "  replace bucket tooth\n",
      "  lost tooth\n",
      "  lost bucket tooth\n",
      "  replace tooth and keepers\n",
      "  lost bucket tooth\n",
      "  lost bucket tooth\n",
      "  replace bucket tooth\n",
      "  replace bucket tooth shd24\n",
      "  busted tooth & pins\n",
      "  replace bucket teeth\n",
      "  replace bucket tooth\n",
      "  replace tooth\n",
      "  replace broken bucket tooth\n",
      "  refit bucket tooth and keepers\n",
      "  repair hyd leak\n",
      "  repair oil leak\n",
      "  hyd hose blown off of cab\n",
      "  repair oil leak\n",
      "  replace hyd leak\n",
      "  replace rh engine cooler fitting\n",
      "  lh engine won't start\n",
      "  jump start machine\n",
      "  jump start\n",
      "  jump start\n",
      "  jump start\n",
      "  jump start\n",
      "  jump start\n",
      "  r/h engine shut down in cab bolts\n",
      "  contamination fault - repairs @ rh side of boom motor\n",
      "  replace rh engine\n",
      "  low oil pressure low\n",
      "  leaking oil fill filters\n",
      "  replace r/h slew motor\n",
      "  hydraulic oil leak @ slew motor\n",
      "  changeout ~ left hand track chain\n",
      "  changeout ~ left hand track chain\n",
      "  changeout rh no-6 track roller - cw\n",
      "  changeout ~ left hand track adjuster\n",
      "  changeout ~ lift propel box\n",
      "  replace hyd hose blown\n",
      "  rh stick cylinder pin coming out\n",
      "  replace r/h slew motor cover pin\n",
      "  replace brake has come off\n",
      "  clamps on hydraulic tank mounts broken\n",
      "  changeout ~ right hand track adjuster\n",
      "  changeout rh no1 track roller - cw\n",
      "  changeout rh no-7 track roller broken.\n",
      "  replace r/h engine top of cab window\n",
      "  changeout ~ right slew gearbox\n",
      "  changeout ~ right hand track adjuster\n",
      "  changeout rh no1 track roller faulty\n",
      "  changeout ~ left hand track chain\n",
      "  change lh start problem\n",
      "  repair oil leak on boom pins\n",
      "  repair hyd leak\n",
      "  hydraulic leak on r/h engine\n",
      "  repair hydraulic oil leak\n",
      "  hydraulic leak on rh engine\n",
      "  lh engine wont start\n",
      "  jump start machine\n",
      "  jump start machine\n",
      "  jump start\n",
      "  jump start machine\n",
      "  jump start\n",
      "  r/h engine won't start\n",
      "  r/h engine starter u/s\n",
      "  lh slew motor hoses broken\n",
      "  changeout rh no1 track roller - cw\n",
      "  changeout rh no-7 track roller - cw\n",
      "  changeout ~ left slew motor hoses\n",
      "  replace rh engine completed\n",
      "  replace rh eng cooler pump sender\n",
      "  repair oil leak at hydraulic tank\n",
      "  replace rh engine cab mounts on shd24\n",
      "  lube system alarm on \n",
      "  replace rhs track adjuster completed\n",
      "  repair leaking hyd timer\n",
      "  replace brake handle cab ex fuel.\n",
      "  replace rh slew motor contamination\n",
      "  fit track pin & replace hose\n",
      "  blown hydraulic hose\n",
      "  replace hyd hose\n",
      "  repair oil leak\n",
      "  reseal #1 main pump case drain hoses\n",
      "  reseal lh travel hoses\n",
      "  repair oil leak at full flow filter\n",
      "  repair hyd leaks\n",
      "  hydraulic tank compressor\n",
      "  cariy out slew motor contamination fault\n",
      "  rh pump gear box getting hot\n",
      "  reseal l/h slew motor hoses\n",
      "  replace seals hyd temp\n",
      "  repairs to coolant fan pump\n",
      "  change out l/h engine\n",
      "  lh engine stop problems\n",
      "  replace blown oring on boom\n",
      "  changeout ~ left hand track chain\n",
      "  changeout ~ right hand slew motor\n",
      "  changeout ~ right slew motor\n",
      "  change lh stick cyl hose\n",
      "  replace r/h slew motor case drain hose\n",
      "  repair hyd leak\n",
      "  repair hydraulic oil leaks\n",
      "  replace rh slew motor hoses\n",
      "  repair oil leak on rh engine\n",
      "  l/h engine won't start\n",
      "  jump start shovel\n",
      "  jump start\n",
      "  required jump start\n",
      "  jump start shovel\n",
      "  jump start\n",
      "  jump start\n",
      "  jump start machine\n",
      "  jump start machine\n",
      "  jump start\n",
      "  jump start\n",
      "  required jump start\n",
      "  jump start\n",
      "  r/h engine shut down in cab\n",
      "  repair oil leak on boom\n",
      "  changeout rh no-2 track roller - cw\n",
      "  changeout rh no-7 track roller - cw\n",
      "  changeout ~ left hand track adjuster\n",
      "  repair oil leak on rh eng aftercooler\n",
      "  replace r/h slew motor\n",
      "  replace hydraulic oil leak\n",
      "  repairs track rollers rhs\n",
      "  repair camera system\n",
      "  rh engine won't start\n",
      "  replace r/h engine overheating\n",
      "  replace r/h engine temp gauges u/s\n",
      "  replace rh slew motor hoses\n",
      "  replace rh eng cooler hyd motor\n",
      "  repair oil leak\n",
      "  hydraulic oil leak\n",
      "  hyd leak on boom pipe\n",
      "  replace blown hydraulic hose\n",
      "  replace r/h slew motor\n",
      "  repair hydraulic oil leak\n",
      "  repair hyd oil leak\n",
      "  replace r/h engine temp gauge u/s\n",
      "  replace l/h eng air filters\n",
      "  repairs to has faulty\n",
      "  repair hyd leak on boom\n",
      "  replace r/h slew motor hoses\n",
      "  replace rh eng cab temp. warning\n",
      "  replace rh eng cameras\n",
      "  replace brake handrails\n",
      "  crack repairs to hyd tank light on\n",
      "  changeout rh no-2 track roller- warranty\n",
      "  c/out lh no-7 track roller - cw\n",
      "  changeout rh no-7 track roller\n",
      "  change out lh servo pump\n",
      "  changeout ~ right slew motor\n",
      "  repair oil leak on boom\n",
      "  repair hydraulic leak\n",
      "  hyd leak at hp filter heads\n",
      "  changeout ~ left hand track adjuster\n",
      "  change out l/h slew motor\n",
      "  replace r/h slew motor\n",
      "  hydraulic oil leak\n",
      "  hydraulic oil leak\n",
      "  replace r/h slew motor hose\n",
      "  reseal l/h pump drive gearboxe\n",
      "  replace r/h slew box\n",
      "  c/o l/h pto-lube pump\n",
      "  replace r/h slew motor hoses\n",
      "  replace rh slew motor changeout\n",
      "  rh120c-rh slew motor changeout\n",
      "  rh120c-rh slew motor changeout\n",
      "  changeout ~ left hand slew gearcase\n",
      "  changeout rh no-7 track roller - cw\n",
      "  rh slew box\n",
      "  replace r/h slew motor hoses\n",
      "  repair oil leak on boom\n",
      "  replace rh eng coolant to plate to be shd24\n",
      "  repair hyd leak on boom\n",
      "  replace broken bucket tooth\n",
      "  replace tooth\n",
      "  replace bucket teeth\n",
      "  replace tooth\n",
      "  repair bucket wear package u/s\n",
      "  repairs to hyd tenp of shd24\n",
      "  lube fault - line b cab bolts\n",
      "  repair oil leak\n",
      "  replace hyd hose\n",
      "  replace hose blown\n",
      "  replace blown hydraulic hose\n",
      "  repair hydraulic oil leak\n",
      "  hyd hose broken/teft\n",
      "  changeout ~ right hand top of boom\n",
      "  repair oil leak on boom\n",
      "  changeout ~ left hand servo pump\n",
      "  change out l/h slew motor mounting broken\n",
      "  change out l/h engine temp gauges\n",
      "  contamination light on u/s\n",
      "  repair camera system\n",
      "  replace rh eng after cooler pipe block\n",
      "  replace broken bolts\n",
      "  blown hyd hose on boom\n",
      "  broken bolts on boom control\n",
      "  replace r/h slew motor hose\n",
      "  replace r/h slew box\n",
      "  replace hydraulic oil leaks\n",
      "  hydraulic tank mounting bolts missing\n",
      "  rh slew motor cover hose\n",
      "  broken grease line on bucket\n",
      "  bucket creeping\n",
      "  repair cracks in cab missing\n",
      "  replace broken bolts\n",
      "  repairs to handrail motor\n",
      "  repair oil leak arm hyd temp sender\n",
      "  changeout rh no-7 track roller\n",
      "  changeout ~ right hand track chain\n",
      "  changeout ~ right slew motor hose\n",
      "  change out r/h servo/pto lube pump\n",
      "  geter servo isolator pump\n",
      "  c/out rh no-1 track roller\n",
      "  replace lh slew motor hose\n",
      "  reseal rh engine cooler hyd motor\n",
      "  leaking fill hose\n",
      "  lh engine top deck cab light on\n",
      "  replace lh slew motor hose\n",
      "  changeout rh no-7 track roller - cw\n",
      "  changeout ~ right servo pump\n",
      "  changeout rh no-7 track roller\n",
      "  change out rh no-7 track roller\n",
      "  changeout ~ left hand track chain\n",
      "  changeout ~ right slew motor\n",
      "  changeout ~ left hand track adjuster\n",
      "  c/out rh no-1 track roller -failed\n",
      "  c/out rh no-1 track roller-warranty\n",
      "  c/out rh no-1 track roller -failed\n",
      "  replace r/h slew missing on boom\n",
      "  replace broken grease lines\n",
      "  repair grease lines on bucket\n",
      "  replace tooth and pin\n",
      "  repair cab light on\n",
      "  changeout rh no-2 track roller - cw\n",
      "  changeout ~ left hand track chain\n",
      "  check air filters completed\n",
      "  replaced reservo in sterting arma\n",
      "  replace brake hoses shd24\n",
      "  replace rh slew motor hoses\n",
      "  repair hyd oil leaks on slew\n",
      "  changeout ~ right slew metar \n",
      "  replace hyd hose brown\n",
      "  changeout rh no1 track roller - cw\n",
      "  changeout ~ left slew motor hose\n",
      "  replace rh slew motor hose\n",
      "  replace rh slew motor has fallen off\n",
      "  repair cameras\n",
      "  repair cracks in both engines over fuel tank blockst blown\n",
      "  repair crack in accump assumbly relay\n",
      "  repair oil leak on boom pins\n",
      "  replace blown hose on hyd starter\n",
      "  replace blown hyd hose.\n",
      "  repair hyd oil leaks\n",
      "  replace rh slew motor cover hoses\n",
      "  reseal l/h slew motor hoses\n",
      "  reseal rh engine fault\n",
      "  rh engine will not start shd24\n",
      "  repairs to hyd hosing on stick\n",
      "  repair cab light on not working\n",
      "  r/h engine starting fault\n",
      "  jump start machine\n",
      "  jump start machine\n",
      "  jump start machine shd0024\n",
      "  jump start machine\n",
      "  jump start shovel\n",
      "  engine wont start\n",
      "  jump start shovel.\n",
      "  jump start\n",
      "  jump start shovel\n",
      "  engine won't start shd24\n",
      "  r/h engine will not start\n",
      "  jump start\n",
      "  jump start machine\n",
      "  jump start shovel\n",
      "  jump start machine\n",
      "  jump start machine\n",
      "  jump start\n",
      "  jump start\n",
      "  replace engine oil pressure light on\n",
      "  changeout rh no-2 track roller-warranty\n",
      "  repair leaking hard to see text\n",
      "  replace seat servo pump ex con stock\n",
      "  replace r/h engine temp. gauge up\n",
      "  coolant leak at hyd hose on starter\n",
      "  replace lh eng start switch u/s\n",
      "  changeout ~ left hand slew gearcase\n",
      "  changeout ~ right hand track chain\n",
      "  changeout ~ left hand track chain chinging vent\n",
      "  change out rh slew motor\n",
      "  change out lh no-6 track roller\n",
      "  c/out rh no-2 track roller - cw\n",
      "  changeout ~ left hand track chain\n",
      "  changeout ~ left slew motor\n",
      "  changeout ~ left servo pump\n",
      "  changeout ~ left hand track chain\n",
      "  change out rh servo pump\n",
      "  change out r/h slew motor\n",
      "  rh engine shutting down light on\n",
      "  changeout rh no-7 track roller - cw\n",
      "  c/out lh no-5 track roller- warranty)\n",
      "  c/out lh no-5 track roller\n",
      "  changeout ~ left hand track adjuster\n",
      "  replace broken hyd hose broken.\n",
      "  repair oil leak on r/h slew motor\n",
      "  changeout rh no1 track roller\n",
      "  changeout ~ left slew motor\n",
      "  changeout lh no-7 track roller - cw\n",
      "  changeout ~ left hand track chain cab mount\n",
      "  repair oil leak at hp filters\n",
      "  replace blown hyd leak on both engines\n",
      "  ivle brake not holding see text\n",
      "  repair hydraulic oil leak\n",
      "  replace hyd hose on boom\n",
      "  replace blown hose\n",
      "  hyd hose leaking\n",
      "  repair oil leak at hydraulic tune up u/s\n",
      "  rh pump gear box getting hot\n",
      "  lost tooth\n",
      "  repair broken grease line on bucket\n",
      "  repair broken grease lines on bucket\n",
      "  bucket cracked\n",
      "  replace blown hyd hose\n",
      "  reservoir pipe contamination switch\n",
      "  replace servo hose leaking\n",
      "  replace blown hydraulic hose\n",
      "  repair hyd leak\n",
      "  reseal #1 pump resealing\n",
      "  replace r/h slew motor\n",
      "  rh eng cameras not working\n",
      "  low out rh slew motor hoses\n",
      "  replace slew motor has fallen off\n",
      "  replace lh eng after cooler pipe broken\n",
      "  replace l/h both murphy switch u/s\n",
      "  changeout ~ left hand servo pump\n",
      "  changeout ~ right slew motor\n",
      "  changeout ~ right slew motor\n",
      "  change out lh slew motor hose\n",
      "  replace rh slew motor\n",
      "  rh120c-lh slew motor\n",
      "  changeout\n",
      "  rh120c-rh slew motor changeout\n",
      "  reseal l/h slew motor\n",
      "  rh slew motor hoses\n",
      "  replace r/h slew motor hose\n",
      "  repair hydraulic leaks\n",
      "  hydraulic tank completed\n",
      "  repair hydraulic oil leak\n",
      "  hyd leak around roller mounting\n",
      "  replace blown hose on hand rails broken.\n",
      "  replace rh slew motor contamination\n",
      "  fit track pin & fit servo pump\n",
      "  repair oil leak on boom\n",
      "  repair cracks in both engines won't start\n",
      "  r/h engine shut down in cab boity seal leaking\n",
      "  replace r/h engine oil filters\n",
      "  replace blown hose on shd24\n",
      "  changeout ~ left slew motor\n",
      "  changeout ~ right slew motor\n",
      "  replace rh slew motor hoses broken\n",
      "  capital - no boom hose\n",
      "  repair hyd leak on boom\n",
      "  changeout ~ left hand turbocharger\n",
      "  reseal lh engine shut down in caming out\n",
      "  repair oil leak\n",
      "  replace r/h engine temp gauge undercarra\n",
      "e repairs\n",
      "  repair oil leak at full flow pipe\n",
      "  replace blown hydraulic hose\n",
      "  repair hyd line\n",
      "  reseal #1 main pump\n",
      "  changeout ~ left hand turbocharger\n",
      "  replace r/h slew motor hoses\n",
      "  replace servo hose\n",
      "  rh slew motor hoses\n",
      "  rh engine cutting out l/h engine overheating\n",
      "  repair hyd oil leaks on boom\n",
      "  repair cracks in cab mounts\n",
      "  changeout rh no1 track roller\n",
      "  change out rh no1 track roller\n",
      "  changeout ~ right hand track chain\n",
      "  changeout ~ left hand turbocharger\n",
      "  change out r/h engine\n",
      "  replace rh slew motor hoses\n",
      "  changeout ~ left hand track adjuster\n",
      "  change out lh no-7 track roller faulty\n",
      "  changeout ~ left hand slew motor\n",
      "  hydraulic leak at hyd leak\n",
      "  repair hyd oil leaks\n",
      "  replace hyd hose\n",
      "  replace hose on stick\n",
      "  replace rh servo pump\n",
      "  changeout ~ left hand track adjuster\n",
      "  repair oil leak at hyd hose blown\n",
      "  repair crack in boom pipe on stick\n",
      "  reseal r/h slew motor hoses blown\n",
      "  changeout ~ left hand slew motor hose\n",
      "  l/h engine temp gauge u/s\n",
      "  changeout ~ left hand track chain\n",
      "  repair oil leak at hp filter block\n",
      "  reseals main strips & camera system\n",
      "  rh pump gear press rocole servo pump\n",
      "  repair l/h engine overheating shd24\n",
      "  replace l/h engine temp sanito box\n",
      "  replace rh engine caming und\n",
      "  contamination alarm coming on.\n",
      "  lh eng oil pressure fault - replate shd24\n",
      "  rh engine cut out coming on\n",
      "  replace rh engine coolant leak\n",
      "  replace r/h engine oil filter heads\n",
      "  changeout ~ left slew motor changeout\n",
      "  repair hyd leak on hp filters\n",
      "  replace broken bolts on cab\n",
      "  repair oil leak on boom\n",
      "  replace broken bucket grease line\n",
      "  repair grease lines on bucket\n",
      "  replace bucket teeth\n",
      "  bucket cracked\n",
      "  contamination alarm\n",
      "  replace blocked broken\n",
      "  replace broken bolts on cab mounts\n",
      "  changeout rh slew motor\n",
      "  rh slew pump mount o ring leaking\n",
      "  replace broken bucket tooth\n",
      "  lost tooth\n",
      "  replace tooth & keeper plets\n",
      "  bucket clamp on stick\n",
      "  lube system fault\n",
      "  grease system fault\n",
      "  lube system fault\n",
      "  lube fault - low pressure shd24\n",
      "  broken bolts on cab mounts\n",
      "  repairs\n",
      "  changeout ~ left hand track adjuster\n",
      "  change out r/h servo/pto lube pump\n",
      "  grease leak on boom\n",
      "  changeout ~ left hand slew gearcase\n",
      "  changeout ~ left hand track chain\n",
      "  change out lh slew motor\n",
      "  repair hydraulic leak\n",
      "  repair hydraulic oil leak\n",
      "  hydraulic oil leak @ steel tube\n",
      "  blown hyd leak on boom\n",
      "  changeout ~ right hand slew motor\n",
      "  changeout ~ left slew motor\n",
      "  changeout ~ right slew motor changeout\n",
      "  reseal r/h pump mounting bolts\n",
      "  rh stick cylinder pin coming out\n",
      "  replace lh slew motor charge hose\n",
      "  changeout ~ left hand track chain\n",
      "  changeout ~ right hand track chain\n",
      "  change out l/h slew motor hoses\n",
      "  replace seals\n",
      "  hyd temp wiring\n",
      "  repair hyd oil leak\n",
      "  hydraulic leak at fuel temp\n",
      "  replace rh slew motor hoses\n",
      "  reseal lh slew motor hose\n",
      "  replace rh slew motor hose\n",
      "  replace horn\n",
      "  rh slew blocking valve\n",
      "  repair hyd oil leak\n",
      "  replace hyd line\n",
      "  repairs to hyd hose bolt off\n",
      "  change out l/h slew motor\n",
      "  replace servo pump main pressure light on\n",
      "  replace l/h engine overflow pipe bracket\n",
      "  bucket teeth and keepers\n",
      "  lost tooth shd24\n",
      "  repairs to hyd hosing\n",
      "  reseal rh slew motor hose\n",
      "  replace hose on r/h engine\n",
      "  rh engine starting fault - warranty\n",
      "  c/out lh no-5 track roller- cw\n",
      "  changeout rh no-7 track roller\n",
      "  changeout ~ left hand track adjuster hose\n",
      "  c/out rh no-1 track roller-failed\n",
      "  repair hydraulic oil leaks\n",
      "  replace rh servo pump\n",
      "  changeout ~ left hand track chain\n",
      "  changeout ~ right slew gearbox\n",
      "  changeout ~ left hand track chain\n",
      "  changeout ~ right hand track adjuster housing\n",
      "  replace rh slew motor hoses\n",
      "  change out r/h servo/pto lube pump\n",
      "  grease sys defalty faulty\n",
      "  replace broken bolts on boom control\n",
      "  servo horn fault\n",
      "  repair hand rails broken\n",
      "  replace blown hose on boom\n",
      "  repair hydraulic oil leak\n",
      "  hydraulic oil leak @ rh engine fittings\n",
      "  change out rh servo pump\n",
      "  changeout ~ left slew motor\n",
      "  repairs to hyd hosing\n",
      "  replace rh slew motor hoses\n",
      "  replace service arm assembly\n",
      "  replace l/h engine temp sender for shd24\n",
      "  reseal r/h pump box\n",
      "  change out l/h slew motor hose\n",
      "  c/out lh no-3 track roller-filter\n",
      "  repair hydraulic oil\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 20000, prime='re', top_k=2, cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rnn.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample(loaded, 2000, cuda=False, top_k=5, prime=\"replace\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

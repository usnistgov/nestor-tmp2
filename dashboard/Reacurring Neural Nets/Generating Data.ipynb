{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesizing Data \n",
    "## By: Lela Bones\n",
    "-----------------------------------\n",
    "### Problem\n",
    "    Nestor is an application that allows users to datify their maintenance logs. Datify is the process of taking unorginized data and putting it into a quantifiable and statistically relevant format. The problem is that because of copywright and data ownership technicalities, it is hard for us to have a large amount of \"good\" data to demo our app on. \n",
    "\n",
    "### Solution\n",
    "    I plan on using Reccurent Neural Nets (RNNs) to generate realistic data from the data that we already have from companies. The reason for this is because we can use the \"synthetic\" data to demo our app on, and we aren't breaking any laws. I plan on using a Python library, Pytorch to implement my RNN and train it on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Pytorch???\n",
    "Pytorch is a framework that builds from the Torch framework that Facebook actively uses. Pytorch is extremely fast it is very native and customizable. Pytorch is also a dynamic deep learning tool, which means that you can change and execute notes as you're learning. This makes RNNs way easier to train because you don't need to set a maximum length and then pad smaller sequences. Debugging is really easy because it is defined at runtime. It works well with Flask, which is the tool that I am using to create my visual dashboard. Pytorch also has declarative data parallelism which allows you to use multiple GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why RNNs???\n",
    "    * Vanilla Neural Networks relearn each iteration\n",
    "    * Long Short Term Memory Neural Nets are good for long-term data\n",
    "    * Maybe GRU Neural Nets, they get rid of the disappearing gradient problem\n",
    "    * RNNs implement loops so the learning is compositional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pandas\n",
    "#conda install pytorch torchvision -c pytorch\n",
    "#conda install numpy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set data file, column you want extracted, and output file\n",
    "csv_file = 'mine_raw.csv'\n",
    "txt_file = 'train.txt'\n",
    "text_col = 'OriginalShorttext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function converts the column of text from your csv file \n",
    "#to a text file that has each row on a seperate line\n",
    "#code modified from https://stackoverflow.com/questions/47339698/how-to-convert-csv-file-to-text-file-using-python\n",
    "def createTextFile(inputFile, text, outputFile):\n",
    "    df = pd.read_csv(inputFile)\n",
    "    data = df[text].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "    text_list = []\n",
    "    for line in data:\n",
    "        text_list.append(\" \".join(line))\n",
    "    \n",
    "    #switch to a+ if you want to append to existing file\n",
    "    with open(outputFile, \"w+\") as output_file:\n",
    "        for line in data:\n",
    "            output_file.write(\"  \" + line + \"\\n\")\n",
    "        #verification that it's finished\n",
    "        print('File Successfully written.')\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Successfully written.\n"
     ]
    }
   ],
   "source": [
    "createTextFile(csv_file, text_col, txt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Attempt\n",
    "The majority of this code was modified from https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 165817\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = unidecode.unidecode(open(txt_file).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "#splitting up the code into random chunks each the size of 200\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "# print(random_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 10,  11,  12,  39,  40,  41])\n"
     ]
    }
   ],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor)\n",
    "\n",
    "print(char_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c].unsqueeze(0))\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / chunk_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltb3/.conda/envs/mwo-lstm/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 12s (100 5%) 2.2991]\n",
      "Wh ang fint out\n",
      "  po/ replace lew ars\n",
      "  r/h  lang ingit\n",
      "  burume mans\n",
      "  r/h en\n",
      "  repl ars osts\n",
      "  relea \n",
      "\n",
      "[0m 25s (200 10%) 2.0789]\n",
      "Wh shd24\n",
      "  ount rh replace replace to replace boower leak sento\n",
      "  2 l/h set repaulinge selt\n",
      "  replace  \n",
      "\n",
      "[0m 36s (300 15%) 1.5288]\n",
      "Whe\n",
      "  replace brokeed\n",
      "  replace - air hd24\n",
      "  repairs\n",
      "  repair alace\n",
      "  r/h engine alder hyd lh fresease \n",
      "\n",
      "[0m 49s (400 20%) 1.2931]\n",
      "Whanged wearair boom syste grease starts\n",
      "  stimers filt steel line\n",
      "  replace pump lh eng\n",
      "  repair aar  \n",
      "\n",
      "[1m 0s (500 25%) 1.5700]\n",
      "Wh engine out sticker engine\n",
      "  reeseal p fuel fire hyd lak buoken\n",
      "  r/h serv hyd reeler hyd and alarme \n",
      "\n",
      "[1m 12s (600 30%) 1.2482]\n",
      "Whing cooler pump\n",
      "  coolank fan fuel on buctorking\n",
      "  replace pill carm ling pinot oil leff pump\n",
      "  ext  \n",
      "\n",
      "[1m 24s (700 35%) 1.1808]\n",
      "Wheng line toout\n",
      "  replace main grease line fault\n",
      "  lose acchine bext\n",
      "  replace mout ~ replace bucket\n",
      " \n",
      "\n",
      "[1m 36s (800 40%) 1.2088]\n",
      "Wh shyd.\n",
      "  replace on lh blocker l.h seal oil terking\n",
      "  replace be cable line oil leaking\n",
      "  thor our t \n",
      "\n",
      "[1m 48s (900 45%) 1.2599]\n",
      "Whd\n",
      "  repair down parter hose shd24 - rh fant pootr\n",
      "  oil leak on hydraulic faul leak an leak\n",
      "  replac \n",
      "\n",
      "[2m 0s (1000 50%) 1.2861]\n",
      "Whear rh engines\n",
      "  recair hose on bucket\n",
      "  replace blown hydraulic tip & boom\n",
      "  replace pump down boom \n",
      "\n",
      "[2m 12s (1100 55%) 0.8111]\n",
      "Wh reass pressurr replace air comms\n",
      "  grease lev on r/h serfeten\n",
      "  replace cab vo bucket grease up\n",
      "  c \n",
      "\n",
      "[2m 25s (1200 60%) 1.1009]\n",
      "Whers on bucket\n",
      "  repair 1 radiose on idler schip\n",
      "  fit rh air con cube fitting on line in treth\n",
      "  oil \n",
      "\n",
      "[2m 39s (1300 65%) 1.4964]\n",
      "Wh joot filters\n",
      "  replace broken grease lever dipair contald @ airs\n",
      "  replace brake plate control leak \n",
      "\n",
      "[2m 52s (1400 70%) 1.1642]\n",
      "Wh pump step\n",
      "  changeout ~ leak at shd24\n",
      "  repair warning boom hose grease fault on hose\n",
      "  replace box \n",
      "\n",
      "[3m 4s (1500 75%) 1.1346]\n",
      "Whine bucket.h.\n",
      "  vece leak l/h engine won't start\n",
      "  blown slew broken stick start\n",
      "  jump start main o \n",
      "\n",
      "[3m 16s (1600 80%) 1.4320]\n",
      "Wh boom\n",
      "  love blube plate rh120 boom pump\n",
      "  cover a/c\n",
      "  replace pin belator lip front lover bel a/c n \n",
      "\n",
      "[3m 28s (1700 85%) 1.3342]\n",
      "Wh boom\n",
      "  replace motor leaking\n",
      "  replace l/h boom fire working on boom\n",
      "  replace r/h see tank valve f \n",
      "\n",
      "[3m 40s (1800 90%) 1.0522]\n",
      "Whs temp on main\n",
      "  air cabine leaking\n",
      "  replace broken grease line motor on bucket teeth shd24\n",
      "  repai \n",
      "\n",
      "[3m 52s (1900 95%) 1.1280]\n",
      "Wh coolant staysing latch\n",
      "  replace cracked coolings in shd24\n",
      "  repair ad pipe to airrolel bucket\n",
      "  re \n",
      "\n",
      "[4m 4s (2000 100%) 0.9737]\n",
      "Whend ination in fault hole\n",
      "  revedss repair ard refitting on lader shd24\n",
      "  replace fuel temp filters  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "lr = 0.005\n",
    "\n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set())       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb72e53c358>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Attempt\n",
    "Most of this code is from https://github.com/mcleonard/pytorch-charRNN/blob/master/TorchRNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "with open (txt_file, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert char to int\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns mini-batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y\n",
    "\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network '''\n",
    "        \n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs\n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        inputs = Variable(torch.from_numpy(x), volatile=True)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([Variable(each.data, volatile=True) for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()),\n",
    "                Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    ''' Traing a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            inputs, targets = Variable(x), Variable(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([Variable(each.data, volatile=True) for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = Variable(x, volatile=True), Variable(y, volatile=True)\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.data[0])\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.data[0]),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltb3/.conda/envs/mwo-lstm/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/ltb3/.conda/envs/mwo-lstm/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/ltb3/.conda/envs/mwo-lstm/lib/python3.6/site-packages/ipykernel_launcher.py:76: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/ltb3/.conda/envs/mwo-lstm/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/ltb3/.conda/envs/mwo-lstm/lib/python3.6/site-packages/ipykernel_launcher.py:87: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25... Step: 10... Loss: 3.1051... Val Loss: 3.1630\n",
      "Epoch: 2/25... Step: 20... Loss: 2.9779... Val Loss: 3.0134\n"
     ]
    }
   ],
   "source": [
    "if 'net' in locals():\n",
    "    del net\n",
    "\n",
    "\n",
    "\n",
    "net = CharRNN(chars, n_hidden=512, n_layers=2)\n",
    "\n",
    "n_seqs, n_steps = 128, 100\n",
    "train(net, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=False, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "with open('rnn.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample(net, 2000, prime='repair', top_k=5, cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rnn.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample(loaded, 2000, cuda=False, top_k=5, prime=\"replace\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks Literature Review\n",
    "### By: Lela Bones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this notebook I plan on including my understanding of Recurrent Neural Networks (RNNs) using multiple readings and python libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notes on \"The Unreasonable Effectiveness of Recurrent Neural Networks\"\n",
    "--------------------------------------\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNNs are better than Vanilla Neural Networks (VNNs) because unlike VNNs, RNNs read sequences of vectors\n",
    "* RNNs are Turning-Complete because they can simulate arbitrary programs\n",
    "* You can process non-sequential data, sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Architecture of RNNS\n",
    "RNNs read in an input vector and ouput a new vector that is dependant on all previous input vectors before the current one"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "rnn = RNN()\n",
    "#x is the input vector\n",
    "#y is the ouput vector\n",
    "y = rnn.step(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* step() is a function that updates the internal state of the RNN. \n",
    "* In a Vaniall RNN the hidden vector would only contain a single layer *h*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example 1\n",
    "Below is a forward pass of a Vanilla RNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "class RNN:\n",
    "  # ...\n",
    "  def step(self, x):\n",
    "    # update the hidden state\n",
    "    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "    # compute the output vector\n",
    "    y = np.dot(self.W_hy, self.h)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* RNN has three parameters which are initialized with random numbers\n",
    "    - matrix W_hh\n",
    "    - matrix W_xh\n",
    "    - matrix W_hy\n",
    "* np.tanh() implements a non-linearity that squashes the activations to the range [-1, 1]\n",
    "* np.dot() does matrix multiplication (dot product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example 2\n",
    "Below is a two layer recurrent network without back propogation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "y1 = rnn1.step(x)\n",
    "y = rnn2.step(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note:** The above implentation is of two separate RNNs where one recieves the input vectors and the second one recieves the output of the first RNN as input. However there is a slightly different, more appilicable RNN known as a *Long Short-Term Memory (LSTM) newtork* that has backpropagation dynamics and a more powerful update equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on \"Understanding LSTM Networks\"\n",
    "----------------\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that RNNs work very well in finishing sentences and completing things when the context is not important. However, when it comes to predicting things with previous data in mind, the RNNs fail to use long term memory to learn. This is where LSTMs come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Idea of LSTMs\n",
    "* First Layer\n",
    "    * sigmoid layer or \"forget gate layer\" that filters what information to keep and not keep (0 is keep none, 1 is keep all)\n",
    "    * $f_t=\\sigma (W_f * [h_{t-1}, x_t] + b_f$\n",
    "* Second Layer\n",
    "    * sigmoid layer or \"input gate layer\" that decides which values get updated\n",
    "    * tanh layer initializes a vector of new candidate values that could be added to the state\n",
    "* Third Layer\n",
    "    * Update the state by combining the previous state with the new one, we do this by muliplying the state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on \"Understanding GRU Networks\"\n",
    "---------------------------------------\n",
    "GRU refers to \"Gated Recurrent Unit\", GRus aim to solve the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
